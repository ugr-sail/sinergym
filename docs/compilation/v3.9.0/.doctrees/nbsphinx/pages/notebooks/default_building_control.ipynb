{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Default building control using an empty action space"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "It is possible to run a simulation using the default building control performed by EnergyPlus (as specified in the building model file).\n",
    "\n",
    "For instance, from the container's workspace, run the following command:\n",
    "\n",
    "\n",
    "```bash\n",
    "$ energyplus -w sinergym/data/weather/USA_PA_Pittsburgh-Allegheny.County.AP.725205_TMY3.epw sinergym/data/buildings/5ZoneAutoDXVAV.epJSON\n",
    "```\n",
    "\n",
    "However, doing this without our framework has some **drawbacks**:\n",
    "\n",
    "- You will only have the default EnergyPlus output and will not have access to **additional output information**, such as data provided by the logger wrapper, which tracks all the environment interactions. \n",
    "\n",
    "- Moreover, building models have a default ``Site:Location``and ``SizingPeriod:DesignDay``, which *Sinergym* automatically adjusts based on the specified weather, so you would need to manually modify these settings before launching the simulation.\n",
    "\n",
    "- Lastly, you would also need to manually adjust the *RunPeriod* in the building file before starting the simulation.\n",
    "\n",
    "Hence, to avoid manual configurations, we recommend setting up an **empty action interface** in a *Sinergym* environment. For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#==============================================================================================#\n",
      "\u001b[38;20m[ENVIRONMENT] (INFO) : Creating Gymnasium environment.\u001b[0m\n",
      "\u001b[38;20m[ENVIRONMENT] (INFO) : Name: Eplus-office-hot-continuous-v1\u001b[0m\n",
      "#==============================================================================================#\n",
      "\u001b[38;20m[MODEL] (INFO) : Working directory created: /workspaces/sinergym/examples/Eplus-office-hot-continuous-v1-res1\u001b[0m\n",
      "\u001b[38;20m[MODEL] (INFO) : Model Config is correct.\u001b[0m\n",
      "\u001b[38;20m[MODEL] (INFO) : Building model Output:Variable updated with defined variable names.\u001b[0m\n",
      "\u001b[38;20m[MODEL] (INFO) : Updated building model Output:Meter with meter names.\u001b[0m\n",
      "\u001b[38;20m[MODEL] (INFO) : Runperiod established.\u001b[0m\n",
      "\u001b[38;20m[MODEL] (INFO) : Episode length (seconds): 31536000.0\u001b[0m\n",
      "\u001b[38;20m[MODEL] (INFO) : timestep size (seconds): 900.0\u001b[0m\n",
      "\u001b[38;20m[MODEL] (INFO) : timesteps per episode: 35040\u001b[0m\n",
      "\u001b[38;20m[REWARD] (INFO) : Reward function initialized.\u001b[0m\n",
      "\u001b[38;20m[ENVIRONMENT] (INFO) : Environment created successfully.\u001b[0m\n",
      "\u001b[38;20m[WRAPPER LoggerWrapper] (INFO) : Wrapper initialized.\u001b[0m\n",
      "#----------------------------------------------------------------------------------------------#\n",
      "\u001b[38;20m[ENVIRONMENT] (INFO) : Starting a new episode.\u001b[0m\n",
      "\u001b[38;20m[ENVIRONMENT] (INFO) : Episode 1: Eplus-office-hot-continuous-v1\u001b[0m\n",
      "#----------------------------------------------------------------------------------------------#\n",
      "\u001b[38;20m[MODEL] (INFO) : Episode directory created.\u001b[0m\n",
      "\u001b[38;20m[MODEL] (INFO) : Weather file USA_AZ_Davis-Monthan.AFB.722745_TMY3.epw used.\u001b[0m\n",
      "\u001b[38;20m[MODEL] (INFO) : Adapting weather to building model.\u001b[0m\n",
      "\u001b[38;20m[ENVIRONMENT] (INFO) : Saving episode output path in /workspaces/sinergym/examples/Eplus-office-hot-continuous-v1-res1/episode-1/output.\u001b[0m\n",
      "\u001b[38;20m[SIMULATOR] (INFO) : handlers initialized.\u001b[0m\n",
      "\u001b[38;20m[SIMULATOR] (INFO) : handlers are ready.\u001b[0m\n",
      "\u001b[38;20m[SIMULATOR] (INFO) : System is ready.\u001b[0m\n",
      "\u001b[38;20m[ENVIRONMENT] (INFO) : Episode 1 started.\u001b[0m\n",
      "Reward:  -3.3966374173223297 {'time_elapsed(hours)': 0.5, 'month': 1, 'day': 1, 'hour': 0, 'is_raining': False, 'action': [], 'timestep': 1, 'reward': -3.3966374173223297, 'energy_term': -0.008430450234156376, 'comfort_term': -3.3882069670881734, 'energy_penalty': -168.6090046831275, 'comfort_penalty': -6.776413934176347, 'total_power_demand': 168.6090046831275, 'total_temperature_violation': 6.776413934176347, 'reward_weight': 0.5}\n",
      "Simulation Progress [Episode 1]:  10%|█         | 10/100 [00:03<00:41,  2.19%/s, 10% completed] Reward:  -28181.260229794305 {'time_elapsed(hours)': 744.25, 'month': 2, 'day': 1, 'hour': 0, 'is_raining': False, 'action': [], 'timestep': 2976, 'reward': -11.282435670662087, 'energy_term': -0.010066938284793056, 'comfort_term': -11.272368732377295, 'energy_penalty': -201.33876569586113, 'comfort_penalty': -22.54473746475459, 'total_power_demand': 201.33876569586113, 'total_temperature_violation': 22.54473746475459, 'reward_weight': 0.5}\n",
      "Simulation Progress [Episode 1]:  17%|█▋        | 17/100 [00:07<00:39,  2.08%/s, 17% completed]Reward:  -45928.442869685314 {'time_elapsed(hours)': 1416.25, 'month': 3, 'day': 1, 'hour': 0, 'is_raining': False, 'action': [], 'timestep': 5664, 'reward': -10.265789928975188, 'energy_term': -0.010066938284793056, 'comfort_term': -10.255722990690396, 'energy_penalty': -201.33876569586113, 'comfort_penalty': -20.511445981380792, 'total_power_demand': 201.33876569586113, 'total_temperature_violation': 20.511445981380792, 'reward_weight': 0.5}\n",
      "Simulation Progress [Episode 1]:  26%|██▌       | 26/100 [00:12<00:38,  1.93%/s, 26% completed]Reward:  -70583.32173267695 {'time_elapsed(hours)': 2160.25, 'month': 4, 'day': 1, 'hour': 0, 'is_raining': False, 'action': [], 'timestep': 8640, 'reward': -12.205084734570114, 'energy_term': -0.010059948897039251, 'comfort_term': -12.195024785673075, 'energy_penalty': -201.198977940785, 'comfort_penalty': -24.39004957134615, 'total_power_demand': 201.198977940785, 'total_temperature_violation': 24.39004957134615, 'reward_weight': 0.5}\n",
      "Simulation Progress [Episode 1]:  34%|███▍      | 34/100 [00:16<00:36,  1.81%/s, 34% completed]Reward:  -114949.57348822395 {'time_elapsed(hours)': 2880.25, 'month': 5, 'day': 1, 'hour': 0, 'is_raining': False, 'action': [], 'timestep': 11520, 'reward': -19.67858584395881, 'energy_term': -0.010059948897039251, 'comfort_term': -19.66852589506177, 'energy_penalty': -201.198977940785, 'comfort_penalty': -39.33705179012354, 'total_power_demand': 201.198977940785, 'total_temperature_violation': 39.33705179012354, 'reward_weight': 0.5}\n",
      "Simulation Progress [Episode 1]:  42%|████▏     | 42/100 [00:21<00:30,  1.89%/s, 42% completed]Reward:  -167275.03895830357 {'time_elapsed(hours)': 3624.25, 'month': 6, 'day': 1, 'hour': 0, 'is_raining': False, 'action': [], 'timestep': 14496, 'reward': -2.216005203576067, 'energy_term': -0.008430450234156376, 'comfort_term': -2.207574753341911, 'energy_penalty': -168.6090046831275, 'comfort_penalty': -4.415149506683822, 'total_power_demand': 168.6090046831275, 'total_temperature_violation': 4.415149506683822, 'reward_weight': 0.5}\n",
      "Simulation Progress [Episode 1]:  51%|█████     | 51/100 [00:26<00:28,  1.72%/s, 51% completed]Reward:  -181092.29637205083 {'time_elapsed(hours)': 4344.25, 'month': 7, 'day': 1, 'hour': 0, 'is_raining': False, 'action': [], 'timestep': 17376, 'reward': -8.06222668623068, 'energy_term': -0.2674115265572015, 'comfort_term': -7.794815159673478, 'energy_penalty': -5348.23053114403, 'comfort_penalty': -15.589630319346956, 'total_power_demand': 5348.23053114403, 'total_temperature_violation': 15.589630319346956, 'reward_weight': 0.5}\n",
      "Simulation Progress [Episode 1]:  59%|█████▉    | 59/100 [00:30<00:23,  1.78%/s, 59% completed]Reward:  -193361.95531817942 {'time_elapsed(hours)': 5088.25, 'month': 8, 'day': 1, 'hour': 0, 'is_raining': False, 'action': [], 'timestep': 20352, 'reward': -5.717796177595927, 'energy_term': -0.010059948897039251, 'comfort_term': -5.707736228698888, 'energy_penalty': -201.198977940785, 'comfort_penalty': -11.415472457397776, 'total_power_demand': 201.198977940785, 'total_temperature_violation': 11.415472457397776, 'reward_weight': 0.5}\n",
      "Simulation Progress [Episode 1]:  67%|██████▋   | 67/100 [00:35<00:17,  1.93%/s, 67% completed]Reward:  -204699.7606145056 {'time_elapsed(hours)': 5832.25, 'month': 9, 'day': 1, 'hour': 0, 'is_raining': False, 'action': [], 'timestep': 23328, 'reward': -3.008607506468884, 'energy_term': -0.008430450234156376, 'comfort_term': -3.000177056234728, 'energy_penalty': -168.6090046831275, 'comfort_penalty': -6.000354112469456, 'total_power_demand': 168.6090046831275, 'total_temperature_violation': 6.000354112469456, 'reward_weight': 0.5}\n",
      "Simulation Progress [Episode 1]:  76%|███████▌  | 76/100 [00:38<00:11,  2.06%/s, 76% completed]Reward:  -215538.97720656803 {'time_elapsed(hours)': 6552.25, 'month': 10, 'day': 1, 'hour': 0, 'is_raining': False, 'action': [], 'timestep': 26208, 'reward': -25.840976922156226, 'energy_term': -0.010059948897039251, 'comfort_term': -25.830916973259185, 'energy_penalty': -201.198977940785, 'comfort_penalty': -51.66183394651837, 'total_power_demand': 201.198977940785, 'total_temperature_violation': 51.66183394651837, 'reward_weight': 0.5}\n",
      "Simulation Progress [Episode 1]:  84%|████████▍ | 84/100 [00:43<00:06,  2.31%/s, 84% completed]Reward:  -258386.53411460613 {'time_elapsed(hours)': 7296.25, 'month': 11, 'day': 1, 'hour': 0, 'is_raining': False, 'action': [], 'timestep': 29184, 'reward': -14.908562902112482, 'energy_term': -0.11418302704063515, 'comfort_term': -14.794379875071847, 'energy_penalty': -2283.660540812703, 'comfort_penalty': -29.588759750143694, 'total_power_demand': 2283.660540812703, 'total_temperature_violation': 29.588759750143694, 'reward_weight': 0.5}\n",
      "Simulation Progress [Episode 1]:  92%|█████████▏| 92/100 [00:46<00:03,  2.43%/s, 92% completed]Reward:  -289807.16297619225 {'time_elapsed(hours)': 8016.25, 'month': 12, 'day': 1, 'hour': 0, 'is_raining': False, 'action': [], 'timestep': 32064, 'reward': -7.385569497126113, 'energy_term': -0.008430450234156376, 'comfort_term': -7.377139046891957, 'energy_penalty': -168.6090046831275, 'comfort_penalty': -14.754278093783913, 'total_power_demand': 168.6090046831275, 'total_temperature_violation': 14.754278093783913, 'reward_weight': 0.5}\n",
      "Simulation Progress [Episode 1]: 100%|██████████| 100/100 [00:49<00:00,  2.41%/s, 100% completed]Episode  0 Mean reward:  -8.927246807673455 Cumulative reward:  -312810.72814087785\n",
      "Simulation Progress [Episode 1]: 100%|██████████| 100/100 [00:51<00:00,  1.93%/s, 100% completed]\n",
      "\u001b[38;20m[ENVIRONMENT] (INFO) : Environment closed. [Eplus-office-hot-continuous-v1]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "import sinergym\n",
    "from sinergym.utils.wrappers import LoggerWrapper\n",
    "\n",
    "env = gym.make(\n",
    "    'Eplus-office-hot-continuous-v1',\n",
    "    actuators={},\n",
    "    action_space=gym.spaces.Box(\n",
    "        low=0,\n",
    "        high=0,\n",
    "        shape=(0,)))\n",
    "env = LoggerWrapper(env)\n",
    "\n",
    "for i in range(1):\n",
    "    obs, info = env.reset()\n",
    "    rewards = []\n",
    "    truncated = terminated = False\n",
    "    current_month = 0\n",
    "    while not (terminated or truncated):\n",
    "        a = env.action_space.sample()\n",
    "        obs, reward, terminated, truncated, info = env.step(a)\n",
    "        rewards.append(reward)\n",
    "        if info['month'] != current_month:  # display results every month\n",
    "            current_month = info['month']\n",
    "            print('Reward: ', sum(rewards), info)\n",
    "    print(\n",
    "        'Episode ',\n",
    "        i,\n",
    "        'Mean reward: ',\n",
    "        np.mean(rewards),\n",
    "        'Cumulative reward: ',\n",
    "        sum(rewards))\n",
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, a default environment is created, but the space and definition of the default action **are replaced with an empty one**. *Sinergym* handles the necessary background changes. Then, the implemented random agent sends **empty actions (`[]`)** to the environment.\n",
    "\n",
    "When setting an empty action space, *Sinergym* retains the default actuators that are defined in the building model. Their complexity and implementation will depend on the building definition in the *epJSON* file.\n",
    "\n",
    "The benefits of this approach include the ability to **mix and match weathers and buildings** as desired, with *Sinergym* automatically making all the necessary configuration.\n",
    "\n",
    "You can simulate as many years as you want in a single experiment, using the pre-defined loggers.\n",
    "\n",
    "This method also provides more flexibility when choosing which observation variables are going to be used. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
