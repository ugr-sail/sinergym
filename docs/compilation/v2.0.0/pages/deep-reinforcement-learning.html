

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Deep Reinforcement Learning Integration &mdash; Sinergym  documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../_static/doc_theme.css?v=23b407c1" />
      <link rel="stylesheet" type="text/css" href="../_static/github_style.css?v=0a3c596a" />

  
    <link rel="shortcut icon" href="../_static/logo-sidebar.png"/>
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=5929fcd5"></script>
      <script src="../_static/doctools.js?v=9bcbadda"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Sinergym with Google Cloud" href="gcloudAPI.html" />
    <link rel="prev" title="Output format" href="output.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #a5beba" >

          
          
          <a href="../index.html" class="icon icon-home">
            Sinergym
              <img src="../_static/logo-sidebar.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
  
              <p class="caption" role="heading"><span class="caption-text">Start Here</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="usage-example.html">Usage example</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">sinergym</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="environments.html">Environments</a></li>
<li class="toctree-l1"><a class="reference internal" href="rewards.html">Rewards</a></li>
<li class="toctree-l1"><a class="reference internal" href="controllers.html">Controllers</a></li>
<li class="toctree-l1"><a class="reference internal" href="wrappers.html">Wrappers</a></li>
<li class="toctree-l1"><a class="reference internal" href="extra-configuration.html">Extra Configuration in Sinergym simulations</a></li>
<li class="toctree-l1"><a class="reference internal" href="output.html">Output format</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Deep Reinforcement Learning Integration</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#drl-logger">DRL Logger</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#tensorboard-structure">Tensorboard structure</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#how-use">How use</a></li>
<li class="toctree-l2"><a class="reference internal" href="#mlflow">Mlflow</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="gcloudAPI.html">Sinergym with Google Cloud</a></li>
<li class="toctree-l1"><a class="reference internal" href="github-actions.html">Github Actions</a></li>
<li class="toctree-l1"><a class="reference internal" href="tests.html">Tests</a></li>
<li class="toctree-l1"><a class="reference internal" href="API-reference.html">API reference</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Examples</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="notebooks/basic_example.html">Basic example</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/change_environment.html">Changing an environment registered in Sinergym</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/wrappers_examples.html">Wrappers example</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/personalize_loggerwrapper.html">Logger Wrapper personalization/configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/rule_controller_example.html">Rule Controller example</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/drl.html">DRL usage example</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/MLflow_example.html">MLFlow example</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/TensorBoard_example.html">TensorBoard example</a></li>
</ul>

  <div class="sidebar-block github-block">
    <h3><i class="fa fa-github"></i> GitHub</h3>
    <ul class="simple">
      <li><a href="https://github.com/ugr-sail/sinergym" target="_blank">
        <i class="fa fa-book"></i> Repository
      </a></li>
      <li><a href="https://github.com/ugr-sail/sinergym/issues" target="_blank">üêû Issues</a></li>
      <li><a href="https://github.com/ugr-sail/sinergym/releases" target="_blank">üöÄ Releases</a></li>
    </ul>
  </div>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu"  style="background: #a5beba" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Sinergym</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Deep Reinforcement Learning Integration</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com/ugr-sail/sinergym/blob/main/docs/source/pages/deep-reinforcement-learning.rst" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="deep-reinforcement-learning-integration">
<h1>Deep Reinforcement Learning Integration<a class="headerlink" href="#deep-reinforcement-learning-integration" title="Link to this heading">ÔÉÅ</a></h1>
<p><em>Sinergym</em> integrates some facilities in order to use <strong>Deep Reinforcement Learning algorithms</strong>
provided by <a class="reference external" href="https://stable-baselines3.readthedocs.io/en/master/">Stable Baselines 3</a>.
Current algorithms checked by <em>Sinergym</em> are:</p>
<table class="docutils align-default">
<tbody>
<tr class="row-odd"><td colspan="4"><p>Stable Baselines 3:</p></td>
</tr>
<tr class="row-even"><td><p>Algorithm</p></td>
<td><p>Discrete</p></td>
<td><p>Continuous</p></td>
<td><p>Type</p></td>
</tr>
<tr class="row-odd"><td><p>PPO</p></td>
<td><p>YES</p></td>
<td><p>YES</p></td>
<td><p>OnPolicyAlgorithm</p></td>
</tr>
<tr class="row-even"><td><p>A2C</p></td>
<td><p>YES</p></td>
<td><p>YES</p></td>
<td><p>OnPolicyAlgorithm</p></td>
</tr>
<tr class="row-odd"><td><p>DQN</p></td>
<td><p>YES</p></td>
<td><p>NO</p></td>
<td><p>OffPolicyAlgorithm</p></td>
</tr>
<tr class="row-even"><td><p>DDPG</p></td>
<td><p>NO</p></td>
<td><p>YES</p></td>
<td><p>OffPolicyAlgorithm</p></td>
</tr>
<tr class="row-odd"><td><p>SAC</p></td>
<td><p>NO</p></td>
<td><p>YES</p></td>
<td><p>OffPolicyAlgorithm</p></td>
</tr>
<tr class="row-even"><td><p>TD3</p></td>
<td><p>NO</p></td>
<td><p>YES</p></td>
<td><p>OffPolicyAlgorithm</p></td>
</tr>
</tbody>
</table>
<p><code class="docutils literal notranslate"><span class="pre">Type</span></code> column has been specified due to its importance about
<em>Stable Baselines callback</em> functionality.</p>
<section id="drl-logger">
<h2>DRL Logger<a class="headerlink" href="#drl-logger" title="Link to this heading">ÔÉÅ</a></h2>
<p><a class="reference external" href="https://stable-baselines3.readthedocs.io/en/master/guide/callbacks.html">Callbacks</a>
are a set of functions that will be called at given <strong>stages of the training procedure</strong>.
You can use callbacks to access internal state of the RL model <strong>during training</strong>.
It allows one to do monitoring, auto saving, model manipulation, progress bars, ‚Ä¶</p>
<p>This structure allows to custom our own logger for DRL executions. Our objective
is to <strong>log all information about our custom environment</strong> specifically.
Therefore, <a class="reference external" href="https://github.com/ugr-sail/sinergym/blob/main/sinergym/utils/callbacks.py">sinergym/sinergym/utils/callbacks.py</a>
has been created with this proposal. Each algorithm has its own differences
about how information is extracted which is why its implementation. <code class="docutils literal notranslate"><span class="pre">LoggerCallback</span></code>
can deal with those subtleties.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You can specify if you want Sinergym logger (see <a class="reference internal" href="output.html#logger"><span class="std std-ref">Logger</span></a>) to record
simulation interactions during training at the same time using
<code class="docutils literal notranslate"><span class="pre">sinergym_logger</span></code> attribute in constructor.</p>
</div>
<p>This callback derives <code class="docutils literal notranslate"><span class="pre">BaseCallback</span></code> from Stable Baselines 3 while <code class="docutils literal notranslate"><span class="pre">BaseCallBack</span></code>
uses <a class="reference external" href="https://www.tensorflow.org/tensorboard?hl=es-419">Tensorboard</a> on the
background at the same time. With <em>Tensorboard</em>, it‚Äôs possible to visualize all DRL
training in real time and compare between different executions. This is an example:</p>
<a class="reference internal image-reference" href="../_images/tensorboard_example.png"><img alt="Tensorboard example" class="align-center" src="../_images/tensorboard_example.png" style="width: 800px;" />
</a>
<p>There are tables which are in some algorithms and not in others and vice versa.
It is important the difference between <code class="docutils literal notranslate"><span class="pre">OnPolicyAlgorithms</span></code> and <code class="docutils literal notranslate"><span class="pre">OffPolicyAlgorithms</span></code>:</p>
<ul class="simple">
<li><p><strong>OnPolicyAlgorithms</strong> can be recorded <strong>each timestep</strong>, we can set a <code class="docutils literal notranslate"><span class="pre">log_interval</span></code> in
learn process in order to specify the <strong>step frequency log</strong>.</p></li>
<li><p><strong>OffPolicyAlgorithms</strong> can be recorded <strong>each episode</strong>. Consequently, <code class="docutils literal notranslate"><span class="pre">log_interval</span></code> in
learn process is used to specify the <strong>episode frequency log</strong> and not step frequency.
Some features like actions and observations are set up in each timestep.
Thus, Off Policy Algorithms record a <strong>mean value</strong> of whole episode values instead
of values steps by steps (see <code class="docutils literal notranslate"><span class="pre">LoggerCallback</span></code> class implementation).</p></li>
</ul>
<section id="tensorboard-structure">
<h3>Tensorboard structure<a class="headerlink" href="#tensorboard-structure" title="Link to this heading">ÔÉÅ</a></h3>
<p>The main structure for <em>Sinergym</em> with <em>Tensorboard</em> is:</p>
<ul>
<li><p><strong>action</strong>: This section has action values during training. When algorithm
is On Policy, it will appear <strong>action_simulation</strong> too. This is because
algorithms in continuous environments has their own output and clipped
with gym action space. Then, this output is parse to simulation action
space (See <a class="reference internal" href="environments.html#observation-action-spaces"><span class="std std-ref">Observation/action spaces</span></a> note box).</p></li>
<li><p><strong>episode</strong>: Here is stored all information about entire episodes.
It is equivalent to <code class="docutils literal notranslate"><span class="pre">progress.csv</span></code> in <em>Sinergym logger</em>
(see <em>Sinergym</em> <a class="reference internal" href="output.html#output-format"><span class="std std-ref">Output format</span></a> section):</p>
<blockquote>
<div><ul class="simple">
<li><p><em>comfort_violation_time(%)</em>: Percentage of time in episode simulation
in which temperature has been out of bound comfort temperature ranges.</p></li>
<li><p><em>cumulative_comfort_penalty</em>: Sum of comfort penalties (reward component)
during whole episode.</p></li>
<li><p><em>cumulative_power</em>: Sum of power consumption during whole episode.</p></li>
<li><p><em>cumulative_power_penalty</em>: Sum of power penalties (reward component)
during whole episode.</p></li>
<li><p><em>cumulative_reward</em>: Sum of reward during whole episode.</p></li>
<li><p><em>ep_length</em>: Timesteps executed in each episode.</p></li>
<li><p><em>mean_comfort_penalty</em>: Mean comfort penalty per step in episode.</p></li>
<li><p><em>mean_power</em>: Mean power consumption per step in episode.</p></li>
<li><p><em>mean_power_penalty</em>: Mean power penalty per step in episode.</p></li>
<li><p><em>mean_reward</em>: Mean reward obtained per step in episode.</p></li>
</ul>
</div></blockquote>
</li>
<li><p><strong>observation</strong>: Here is recorded all observation values during simulation.
This values depends on the environment which is being simulated
(see <a class="reference internal" href="environments.html#observation-action-spaces"><span class="std std-ref">Observation/action spaces</span></a> section).</p></li>
<li><p><strong>normalized_observation</strong> (optional): This section appear only when environment
has been <strong>wrapped with normalization</strong> (see <a class="reference internal" href="wrappers.html#wrappers"><span class="std std-ref">Wrappers</span></a> section). The model
will train with this normalized values and they will be recorded both;
original observation and normalized observation.</p></li>
<li><p><strong>rollout</strong>: Algorithm metrics in <strong>Stable Baselines by default</strong>. For example,
DQN has <code class="docutils literal notranslate"><span class="pre">exploration_rate</span></code> and this value doesn‚Äôt appear in other algorithms.</p></li>
<li><p><strong>time</strong>: Monitoring time of execution.</p></li>
<li><p><strong>train</strong>: Record specific neural network information for each algorithm,
provided by <strong>Stable baselines</strong> as well as rollout.</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Evaluation of models can be recorded too, adding <code class="docutils literal notranslate"><span class="pre">EvalLoggerCallback</span></code>
to model learn method.</p>
</div>
</section>
</section>
<section id="how-use">
<h2>How use<a class="headerlink" href="#how-use" title="Link to this heading">ÔÉÅ</a></h2>
<p>You can try your own experiments and benefit from this functionality.
<a class="reference external" href="https://github.com/ugr-sail/sinergym/blob/main/scripts/DRL_battery.py">sinergym/scripts/DRL_battery.py</a>
is a example code to use it. You can use <code class="docutils literal notranslate"><span class="pre">DRL_battery.py</span></code> directly from
your local computer specifying <code class="docutils literal notranslate"><span class="pre">--tensorboard</span></code> flag in execution.</p>
<p>The most <strong>important information</strong> you must keep in mind when you try
your own experiments are:</p>
<ul class="simple">
<li><p>Model is constructed with a algorithm constructor.
Each algorithm can use its <strong>particular parameters</strong>.</p></li>
<li><p>If you wrapper environment with normalization, models
will <strong>train</strong> with those <strong>normalized</strong> values.</p></li>
<li><p>Callbacks can be <strong>concatenated</strong> in a <code class="docutils literal notranslate"><span class="pre">CallbackList</span></code>
instance from Stable Baselines 3.</p></li>
<li><p>Neural network will not train until you execute
<code class="docutils literal notranslate"><span class="pre">model.learn()</span></code> method. Here is where you
specify train <code class="docutils literal notranslate"><span class="pre">timesteps</span></code>, <code class="docutils literal notranslate"><span class="pre">callbacks</span></code> and <code class="docutils literal notranslate"><span class="pre">log_interval</span></code>
as we commented in type algorithms (On and Off Policy).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">DRL_battery.py</span></code> requires some <strong>extra arguments</strong> to being
executed like <code class="docutils literal notranslate"><span class="pre">-env</span></code> and <code class="docutils literal notranslate"><span class="pre">-ep</span></code>.</p></li>
<li><p>You can execute <strong>Curriculum Learning</strong>, you only have to
add <code class="docutils literal notranslate"><span class="pre">--model</span></code> field with a valid model path, this script
will load the model and execute to train.</p></li>
</ul>
</section>
<section id="mlflow">
<h2>Mlflow<a class="headerlink" href="#mlflow" title="Link to this heading">ÔÉÅ</a></h2>
<p>Our scripts to run DRL with <em>Sinergym</em> environments are using
<a class="reference external" href="https://mlflow.org/">Mlflow</a> in order to <strong>tracking experiments</strong>
and recorded them methodically. It is recommended to use it.
You can start a local server with information stored during the
battery of experiments such as initial and ending date of execution,
hyperparameters, duration, etc.</p>
<p>Here is an example:</p>
<a class="reference internal image-reference" href="../_images/mlflow_example.png"><img alt="Tensorboard example" class="align-center" src="../_images/mlflow_example.png" style="width: 800px;" />
</a>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For information about how use <em>Tensorboard</em> and <em>Mlflow</em> with a Cloud
Computing paradigm, see <a class="reference internal" href="gcloudAPI.html#remote-tensorboard-log"><span class="std std-ref">Remote Tensorboard log</span></a> and
<a class="reference internal" href="gcloudAPI.html#mlflow-tracking-server-set-up"><span class="std std-ref">Mlflow tracking server set up</span></a>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><em>This is a work in progress project. Direct support with others
algorithms is being planned for the future!</em></p>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="output.html" class="btn btn-neutral float-left" title="Output format" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="gcloudAPI.html" class="btn btn-neutral float-right" title="Sinergym with Google Cloud" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, J. Jim√©nez, J. G√≥mez, M. Molina, A. Manjavacas, A. Campoy.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  
<div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
    <span class="rst-current-version" data-toggle="rst-current-version">
        <span class="fa fa-book"> Other Versions</span>
        v: v2.0.0
        <span class="fa fa-caret-down"></span>
    </span>
    <div class="rst-other-versions">
        <dl>
            <dt>Tags</dt>
            <dd><a href="../../v1.4.0/pages/deep-reinforcement-learning.html">v1.4.0</a></dd>
            <dd><a href="../../v1.6.0/pages/deep-reinforcement-learning.html">v1.6.0</a></dd>
            <dd><a href="../../v1.7.0/pages/deep-reinforcement-learning.html">v1.7.0</a></dd>
            <dd><a href="deep-reinforcement-learning.html">v2.0.0</a></dd>
            <dd><a href="../../v2.1.0/pages/deep-reinforcement-learning.html">v2.1.0</a></dd>
            <dd><a href="../../v2.2.0/pages/deep-reinforcement-learning.html">v2.2.0</a></dd>
            <dd><a href="../../v2.3.0/pages/deep-reinforcement-learning.html">v2.3.0</a></dd>
            <dd><a href="../../v2.5.0/pages/deep-reinforcement-learning.html">v2.5.0</a></dd>
            <dd><a href="../../v3.1.0/pages/deep-reinforcement-learning.html">v3.1.0</a></dd>
            <dd><a href="../../v3.2.0/pages/deep-reinforcement-learning.html">v3.2.0</a></dd>
            <dd><a href="../../v3.3.0/pages/deep-reinforcement-learning.html">v3.3.0</a></dd>
            <dd><a href="../../v3.4.0/pages/deep-reinforcement-learning.html">v3.4.0</a></dd>
            <dd><a href="../../v3.5.0/pages/deep-reinforcement-learning.html">v3.5.0</a></dd>
            <dd><a href="../../v3.6.0/pages/deep-reinforcement-learning.html">v3.6.0</a></dd>
            <dd><a href="../../v3.7.0/pages/deep-reinforcement-learning.html">v3.7.0</a></dd>
            <dd><a href="../../v3.8.0/pages/deep-reinforcement-learning.html">v3.8.0</a></dd>
            <dd><a href="../../v3.9.0/pages/deep-reinforcement-learning.html">v3.9.0</a></dd>
        </dl>
        <dl>
            <dt>Branches</dt>
            <dd><a href="../../main/pages/deep-reinforcement-learning.html">main</a></dd>
        </dl>
    </div>
</div><script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>