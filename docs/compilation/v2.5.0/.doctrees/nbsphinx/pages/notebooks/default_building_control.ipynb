{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Default building control setting up an empty action interface"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "When you want to run a simulation with all the default building controllers (specified in the building file). You can directly use the **EnergyPlus simulation engine**. For example, placing us in the workspace of the container would be to run the following:\n",
    "\n",
    "```bash\n",
    "$ energyplus -w sinergym/data/weather/USA_PA_Pittsburgh-Allegheny.County.AP.725205_TMY3.epw sinergym/data/buildings/5ZoneAutoDXVAV.epJSON\n",
    "```\n",
    "\n",
    "However, doing so without our framework has some **disadvantages**. You will have EnergyPlus default output and will not have some **added outputs** such as our wrapper logger that monitors all interactions with the environment. The buildings have a default *Site:Location* and *SizingPeriod:DesignDay*, which Sinergym changes automatically depending on the specified weather, so you would have to **adjust it** before using the simulator manually. Finally, the *RunPeriod* of the buildings are set to one episode for DRL, which means that as the buildings stand you can only simulate one year. So, you would also have to modify the *RunPeriod* **manually** in the building file before starting the simulation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, our recommended proposal is setting up an **empty action interface** in an environment with Sinergym. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-05-26 08:26:21,986] EPLUS_ENV_office-hot-continuous-v1_MainThread_ROOT INFO:Updating Building model ExternalInterface object if it is not present...\n",
      "[2023-05-26 08:26:21,987] EPLUS_ENV_office-hot-continuous-v1_MainThread_ROOT INFO:Updating Building model Site:Location and SizingPeriod:DesignDay(s) to weather and ddy file...\n",
      "[2023-05-26 08:26:21,990] EPLUS_ENV_office-hot-continuous-v1_MainThread_ROOT INFO:Updating building model OutPut:Variable and variables XML tree model for BVCTB connection.\n",
      "[2023-05-26 08:26:21,992] EPLUS_ENV_office-hot-continuous-v1_MainThread_ROOT INFO:Setting up extra configuration in building model if exists...\n",
      "[2023-05-26 08:26:21,992] EPLUS_ENV_office-hot-continuous-v1_MainThread_ROOT INFO:Setting up action definition in building model if exists...\n",
      "[2023-05-26 08:26:21,994] EPLUS_ENV_office-hot-continuous-v1_MainThread_ROOT INFO:Creating new EnergyPlus simulation episode...\n",
      "[2023-05-26 08:26:22,125] EPLUS_ENV_office-hot-continuous-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /workspaces/sinergym/examples/Eplus-env-office-hot-continuous-v1-res2/Eplus-env-sub_run1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/opyplus/weather_data/weather_data.py:493: FutureWarning: the 'line_terminator'' keyword is deprecated, use 'lineterminator' instead.\n",
      "  epw_content = self._headers_to_epw(use_datetimes=use_datetimes) + df.to_csv(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward:  -0.008417052432114895 {'timestep': 1, 'time_elapsed': 900, 'year': 1991, 'month': 1, 'day': 1, 'hour': 0, 'action': [], 'reward': -0.008417052432114895, 'reward_energy': -0.01683410486422979, 'reward_comfort': -0.0, 'total_energy': 168.3410486422979, 'abs_comfort': 0.0, 'temperatures': [22.53521983917315, 19.55815762759406, 22.14538994164463, 22.09202095142567, 23.0089130888504, 22.04760361521357, 19.67298455304227, 20.01494159872766, 21.50709678475458, 20.5036982527685, 19.79813396884532, 20.07783653111332, 21.2899462240987, 20.52024603483249, 20.44966114692166, 20.79116823683051, 22.25621159111793, 21.27845569962855]}\n",
      "Reward:  -1434.9428376416104 {'timestep': 2976, 'time_elapsed': 2678400, 'year': 1991, 'month': 2, 'day': 1, 'hour': 0, 'action': [], 'reward': -0.012472211059939661, 'reward_energy': -0.024944422119879323, 'reward_comfort': -0.0, 'total_energy': 249.4442211987932, 'abs_comfort': 0.0, 'temperatures': [23.95529447712287, 22.93330599357152, 24.91120914630909, 24.34759748424246, 25.60529980656519, 25.17241292870662, 23.56088321676432, 23.72285660474848, 24.48878476857699, 24.06823112590142, 22.33229321299511, 22.53941488968512, 23.21224560003259, 22.86950118207466, 23.87860676740183, 24.03504108838575, 24.78653292554888, 24.37516306525545]}\n",
      "Reward:  -2098.1863449693465 {'timestep': 5664, 'time_elapsed': 5097600, 'year': 1991, 'month': 3, 'day': 1, 'hour': 0, 'action': [], 'reward': -0.012472211059939661, 'reward_energy': -0.024944422119879323, 'reward_comfort': -0.0, 'total_energy': 249.4442211987932, 'abs_comfort': 0.0, 'temperatures': [23.89537368460365, 22.73228573282486, 24.78319753474181, 24.22081948801992, 25.50517507903195, 25.02949560756553, 23.30320823246185, 23.53016106638053, 24.38676574975518, 23.95899166945338, 22.1197378939569, 22.37779165374341, 23.00186172439464, 22.80962279202423, 23.65232271388191, 23.87300159773843, 24.71858098544965, 24.30061854058779]}\n",
      "Reward:  -3387.5960212991654 {'timestep': 8640, 'time_elapsed': 7776000, 'year': 1991, 'month': 4, 'day': 1, 'hour': 0, 'action': [], 'reward': -0.010059948897039251, 'reward_energy': -0.020119897794078502, 'reward_comfort': -0.0, 'total_energy': 201.198977940785, 'abs_comfort': 0.0, 'temperatures': [23.15118892934099, 22.68245717396391, 24.24402157474289, 23.42391845372941, 24.55701334690791, 24.52489516521943, 23.14721794427982, 23.70576735867768, 24.21813840249226, 24.17410288882376, 21.65263744663373, 22.15637165561054, 22.67644678622419, 22.73343265237197, 23.12290932041643, 23.70313572692205, 24.26957472797558, 24.23273321762333]}\n",
      "Reward:  -6511.303282459102 {'timestep': 11520, 'time_elapsed': 10368000, 'year': 1991, 'month': 5, 'day': 1, 'hour': 0, 'action': [], 'reward': -0.010059948897039251, 'reward_energy': -0.020119897794078502, 'reward_comfort': -0.0, 'total_energy': 201.198977940785, 'abs_comfort': 0.0, 'temperatures': [24.05264115641391, 24.44729521170201, 25.64918371904614, 24.89887704115841, 25.95299993503936, 25.85657110200199, 25.0783983820759, 25.35999783387461, 25.39475723084232, 25.81166145597138, 23.41449827024159, 23.67206860778739, 23.77440975962447, 24.16817346431958, 25.13722363182544, 25.38254171666813, 25.41823646286382, 25.85447471137039]}\n",
      "Reward:  -10535.097308720588 {'timestep': 14496, 'time_elapsed': 13046400, 'year': 1991, 'month': 6, 'day': 1, 'hour': 0, 'action': [], 'reward': -0.008430450234156376, 'reward_energy': -0.01686090046831275, 'reward_comfort': -0.0, 'total_energy': 168.6090046831275, 'abs_comfort': 0.0, 'temperatures': [24.10299314796017, 25.29764272369417, 25.92771625840211, 25.18177330280738, 26.00327277795856, 26.06405171899508, 26.02666361701911, 26.24966082016038, 26.03904879581279, 26.85370369344004, 24.23350556151528, 24.39245729803426, 24.29184088039651, 25.02518258185097, 25.98891383439227, 26.19014861710788, 25.97828895622849, 26.78100217713515]}\n",
      "Reward:  -17052.899318195246 {'timestep': 17376, 'time_elapsed': 15638400, 'year': 1991, 'month': 7, 'day': 1, 'hour': 0, 'action': [], 'reward': -0.23643866755706006, 'reward_energy': -0.020119897794078502, 'reward_comfort': -0.4527574373200416, 'total_energy': 201.198977940785, 'abs_comfort': 0.4527574373200416, 'temperatures': [25.18285854434887, 26.74872894948834, 26.61189462978766, 26.02086591490914, 25.18184212415447, 26.08560769497404, 26.74962263942351, 26.94705967500637, 26.7662014322217, 27.43423229548982, 25.97855387827666, 26.17288182018041, 25.99294064308231, 26.78120533848021, 26.20917317875767, 26.44762895440854, 26.2262496513037, 27.01852514183022]}\n",
      "Reward:  -22936.51953423165 {'timestep': 20352, 'time_elapsed': 18316800, 'year': 1991, 'month': 8, 'day': 1, 'hour': 0, 'action': [], 'reward': -0.018276601381479952, 'reward_energy': -0.020119897794078502, 'reward_comfort': -0.0164333049688814, 'total_energy': 201.198977940785, 'abs_comfort': 0.0164333049688814, 'temperatures': [25.5392169509091, 26.16873133304136, 26.08066550485968, 25.83761436729132, 25.8990371935079, 26.14511417944243, 26.53842299633071, 26.86812113755267, 26.75272617967185, 27.01643330496888, 25.83599947277107, 26.13115881226312, 26.15878336706238, 26.30269066602163, 26.30247967540778, 26.61790313741343, 26.50780463172365, 26.7776207692181]}\n",
      "Reward:  -28546.69328617454 {'timestep': 23328, 'time_elapsed': 20995200, 'year': 1991, 'month': 9, 'day': 1, 'hour': 0, 'action': [], 'reward': -0.008430450234156376, 'reward_energy': -0.01686090046831275, 'reward_comfort': -0.0, 'total_energy': 168.6090046831275, 'abs_comfort': 0.0, 'temperatures': [25.37560489470522, 25.79474684837675, 25.86698199576748, 25.43363330657174, 26.02366936329494, 26.20719535073523, 26.2989879011191, 26.44670657334948, 26.46931843617206, 26.54775608161636, 25.2344308473864, 25.37859048511651, 25.40024332035621, 25.48304453380965, 26.1537180471515, 26.3032686362277, 26.32247978037293, 26.41230402548253]}\n",
      "Reward:  -33156.538182009026 {'timestep': 26208, 'time_elapsed': 23587200, 'year': 1991, 'month': 10, 'day': 1, 'hour': 0, 'action': [], 'reward': -0.010059948897039251, 'reward_energy': -0.020119897794078502, 'reward_comfort': -0.0, 'total_energy': 201.198977940785, 'abs_comfort': 0.0, 'temperatures': [25.44123883033579, 25.41619977445056, 25.88987627436478, 25.56828870185858, 26.19812063166409, 26.13931485589389, 26.01020583080124, 26.19149589468718, 26.26855668465098, 26.21149845430837, 25.28885051351978, 25.46710088969173, 25.61904062266798, 25.5053008724018, 26.06650285483307, 26.24231755447375, 26.31755406341168, 26.25921261409608]}\n",
      "Reward:  -35863.62199140798 {'timestep': 29184, 'time_elapsed': 26265600, 'year': 1991, 'month': 11, 'day': 1, 'hour': 0, 'action': [], 'reward': -0.010059948897039251, 'reward_energy': -0.020119897794078502, 'reward_comfort': -0.0, 'total_energy': 201.198977940785, 'abs_comfort': 0.0, 'temperatures': [24.1877495160313, 23.39942018132908, 25.18171280832089, 24.66871018308847, 25.68478896373198, 25.25242446113143, 24.0090532494292, 24.20737458330498, 25.06701740467029, 24.53709590781486, 22.90144381773696, 23.12430768198516, 23.88987656848913, 23.45426453532504, 24.32224710528407, 24.51629741435834, 25.37530092061942, 24.84193011583043]}\n",
      "Reward:  -37237.62787266026 {'timestep': 32064, 'time_elapsed': 28857600, 'year': 1991, 'month': 12, 'day': 1, 'hour': 0, 'action': [], 'reward': -0.008430450234156376, 'reward_energy': -0.01686090046831275, 'reward_comfort': -0.0, 'total_energy': 168.6090046831275, 'abs_comfort': 0.0, 'temperatures': [22.95210497536126, 21.57040873358834, 23.99129691926893, 23.39186611484875, 24.51112886488032, 23.88961169723077, 22.10093436763699, 22.35990857059858, 23.237994001168, 22.41603845689724, 21.07804657515512, 21.33953654945672, 22.13313673910118, 21.43223892396855, 22.53463662217912, 22.77811569998267, 23.67095962186454, 22.85581406682531]}\n",
      "Reward:  -38261.61061545389 {'timestep': 35040, 'time_elapsed': 31536000, 'year': 1992, 'month': 1, 'day': 1, 'hour': 0, 'action': [], 'reward': -0.012472211059939661, 'reward_energy': -0.024944422119879323, 'reward_comfort': -0.0, 'total_energy': 249.4442211987932, 'abs_comfort': 0.0, 'temperatures': [23.73107679421617, 22.70481166778426, 24.35526077997812, 23.88154331497665, 25.09980439304872, 24.66852229989068, 23.37519187454117, 23.51389693484603, 24.0909126946236, 23.57393397351987, 22.35685000789159, 22.52393219496679, 22.92907195172767, 22.58720938358775, 23.72567767311014, 23.84174091434151, 24.4370711295133, 23.91811470398408]}\n",
      "Episode  0 Mean reward:  -1.0919409422220332 Cumulative reward:  -38261.61061545389\n",
      "[2023-05-26 08:27:01,448] EPLUS_ENV_office-hot-continuous-v1_MainThread_ROOT INFO:EnergyPlus simulation closed successfully. \n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "import sinergym\n",
    "from sinergym.utils.wrappers import LoggerWrapper\n",
    "\n",
    "env = gym.make(\n",
    "    'Eplus-office-hot-continuous-v1',\n",
    "    action_variables=[],\n",
    "    action_space=gym.spaces.Box(\n",
    "        low=0,\n",
    "        high=0,\n",
    "        shape=(0,)),\n",
    "    action_definition=None)\n",
    "env = LoggerWrapper(env)\n",
    "\n",
    "for i in range(1):\n",
    "    obs, info = env.reset()\n",
    "    rewards = []\n",
    "    terminated = False\n",
    "    current_month = 0\n",
    "    while not terminated:\n",
    "        a = env.action_space.sample()\n",
    "        obs, reward, terminated, truncated, info = env.step(a)\n",
    "        rewards.append(reward)\n",
    "        if info['month'] != current_month:  # display results every month\n",
    "            current_month = info['month']\n",
    "            print('Reward: ', sum(rewards), info)\n",
    "    print(\n",
    "        'Episode ',\n",
    "        i,\n",
    "        'Mean reward: ',\n",
    "        np.mean(rewards),\n",
    "        'Cumulative reward: ',\n",
    "        sum(rewards))\n",
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, a default environment is loaded, but the space and definition of the default action **is replaced with an empty one**, Sinergym takes care of making the necessary changes in the background. Then, the random agent implemented send **empty actions ([])**.\n",
    "\n",
    "The advantages are that you can **combine the weathers with the buildings** as you want, Sinergym will take care of adapting everything automatically (you don't have the disadvantages of before), you can run in a single experiment as many years as you want, with our loggers. When you set an empty action interface, Sinergym preserves the default actuators that the building definition comes with (these can be more or less sophisticated depending on the definition of the building in the *epJSON* file)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
