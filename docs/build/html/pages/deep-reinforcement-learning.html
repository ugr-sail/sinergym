<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Deep Reinforcement Learning Integration &mdash; sinergym  documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
    <link rel="shortcut icon" href="../_static/logo-sidebar.png"/>
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Sinergym with Google Cloud" href="gcloudAPI.html" />
    <link rel="prev" title="Wrappers" href="wrappers.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #a9c1be" >
            <a href="../index.html" class="icon icon-home"> sinergym
            <img src="../_static/logo-sidebar.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="environments.html">Environments</a></li>
<li class="toctree-l1"><a class="reference internal" href="github-actions.html">Github Actions</a></li>
<li class="toctree-l1"><a class="reference internal" href="tests.html">Tests</a></li>
<li class="toctree-l1"><a class="reference internal" href="usage-example.html">Usage example</a></li>
<li class="toctree-l1"><a class="reference internal" href="extra-configuration.html">Extra Configuration in Sinergym simulations</a></li>
<li class="toctree-l1"><a class="reference internal" href="output.html">Output format</a></li>
<li class="toctree-l1"><a class="reference internal" href="rewards.html">Rewards</a></li>
<li class="toctree-l1"><a class="reference internal" href="controllers.html">Controllers</a></li>
<li class="toctree-l1"><a class="reference internal" href="wrappers.html">Wrappers</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Deep Reinforcement Learning Integration</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#drl-logger">DRL Logger</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#tensorboard-structure">Tensorboard structure</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#how-use">How use</a></li>
<li class="toctree-l2"><a class="reference internal" href="#mlflow">Mlflow</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="gcloudAPI.html">Sinergym with Google Cloud</a></li>
<li class="toctree-l1"><a class="reference internal" href="API-reference.html">API reference</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu"  style="background: #a9c1be" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">sinergym</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Deep Reinforcement Learning Integration</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/pages/deep-reinforcement-learning.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="deep-reinforcement-learning-integration">
<h1>Deep Reinforcement Learning Integration<a class="headerlink" href="#deep-reinforcement-learning-integration" title="Permalink to this headline"></a></h1>
<p>Sinergym integrates some facilities in order to use Deep Reinforcement Learning algorithms provided by <a class="reference external" href="https://stable-baselines3.readthedocs.io/en/master/">Stable Baselines 3</a>.
Current algorithms checked by Sinergym are:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 21%" />
<col style="width: 19%" />
<col style="width: 23%" />
<col style="width: 38%" />
</colgroup>
<tbody>
<tr class="row-odd"><td colspan="4"><p>Stable Baselines 3:</p></td>
</tr>
<tr class="row-even"><td><p>Algorithm</p></td>
<td><p>Discrete</p></td>
<td><p>Continuous</p></td>
<td><p>Type</p></td>
</tr>
<tr class="row-odd"><td><p>PPO</p></td>
<td><p>YES</p></td>
<td><p>YES</p></td>
<td><p>OnPolicyAlgorithm</p></td>
</tr>
<tr class="row-even"><td><p>A2C</p></td>
<td><p>YES</p></td>
<td><p>YES</p></td>
<td><p>OnPolicyAlgorithm</p></td>
</tr>
<tr class="row-odd"><td><p>DQN</p></td>
<td><p>YES</p></td>
<td><p>NO</p></td>
<td><p>OffPolicyAlgorithm</p></td>
</tr>
<tr class="row-even"><td><p>DDPG</p></td>
<td><p>NO</p></td>
<td><p>YES</p></td>
<td><p>OffPolicyAlgorithm</p></td>
</tr>
<tr class="row-odd"><td><p>SAC</p></td>
<td><p>NO</p></td>
<td><p>YES</p></td>
<td><p>OffPolicyAlgorithm</p></td>
</tr>
<tr class="row-even"><td><p>TD3</p></td>
<td><p>NO</p></td>
<td><p>YES</p></td>
<td><p>OffPolicyAlgorithm</p></td>
</tr>
</tbody>
</table>
<p><code class="docutils literal notranslate"><span class="pre">Type</span></code> column has been specified due to its importance about <em>Stable Baselines callback</em> functionality.</p>
<section id="drl-logger">
<h2>DRL Logger<a class="headerlink" href="#drl-logger" title="Permalink to this headline"></a></h2>
<p><a class="reference external" href="https://stable-baselines3.readthedocs.io/en/master/guide/callbacks.html">Callbacks</a> are a set of functions that will
be called at given stages of the training procedure. You can use callbacks to access internal state of the RL model during training.
It allows one to do monitoring, auto saving, model manipulation, progress bars, ...</p>
<p>This structure allows to custom our own logger for DRL executions. Our objective is to <strong>log all information about our custom environment</strong> specifically.
Therefore, <a class="reference external" href="https://github.com/jajimer/sinergym/blob/main/sinergym/utils/callbacks.py">sinergym/sinergym/utils/callbacks.py</a> has been created with this proposal.
Each algorithm has its own differences about how information is extracted which is why its implementation. <code class="docutils literal notranslate"><span class="pre">LoggerCallback</span></code> can deal with those subtleties.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">LoggerCallback</span><span class="p">(</span><span class="n">BaseCallback</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Custom callback for plotting additional values in tensorboard.</span>
<span class="sd">        :param ep_rewards: Here will be stored all rewards during episode.</span>
<span class="sd">        :param ep_powers: Here will be stored all consumption data during episode.</span>
<span class="sd">        :param ep_term_comfort: Here will be stored all comfort terms (reward component) during episode.</span>
<span class="sd">        :param ep_term_energy: Here will be stored all energy terms (reward component) during episode.</span>
<span class="sd">        :param num_comfort_violation: Number of timesteps in which comfort has been violated.</span>
<span class="sd">        :param ep_timesteps: Each timestep during an episode, this value increment 1.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sinergym_logger</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Custom callback for plotting additional values in tensorboard.</span>

<span class="sd">        Args:</span>
<span class="sd">            sinergym_logger (boolean): Indicate if CSVLogger inner Sinergym will be activated or not.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LoggerCallback</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">verbose</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">sinergym_logger</span> <span class="o">=</span> <span class="n">sinergym_logger</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">ep_rewards</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ep_powers</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ep_term_comfort</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ep_term_energy</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_comfort_violation</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ep_timesteps</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">def</span> <span class="nf">_on_training_start</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># sinergym logger</span>
        <span class="k">if</span> <span class="n">is_wrapped</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">training_env</span><span class="p">,</span> <span class="n">LoggerWrapper</span><span class="p">):</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">sinergym_logger</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">training_env</span><span class="o">.</span><span class="n">env_method</span><span class="p">(</span><span class="s1">&#39;activate_logger&#39;</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">training_env</span><span class="o">.</span><span class="n">env_method</span><span class="p">(</span><span class="s1">&#39;deactivate_logger&#39;</span><span class="p">)</span>

        <span class="c1"># record method depending on the type of algorithm</span>

        <span class="k">if</span> <span class="s1">&#39;OnPolicyAlgorithm&#39;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">globals</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">record</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">record</span>
        <span class="k">elif</span> <span class="s1">&#39;OffPolicyAlgorithm&#39;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">globals</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">record</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">record_mean</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">KeyError</span>

    <span class="k">def</span> <span class="nf">_on_step</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="n">info</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">locals</span><span class="p">[</span><span class="s1">&#39;infos&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

        <span class="c1"># OBSERVATION</span>
        <span class="n">variables</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_env</span><span class="o">.</span><span class="n">get_attr</span><span class="p">(</span><span class="s1">&#39;variables&#39;</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;observation&#39;</span><span class="p">]</span>
        <span class="c1"># log normalized and original values</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_env</span><span class="o">.</span><span class="n">env_is_wrapped</span><span class="p">(</span>
                <span class="n">wrapper_class</span><span class="o">=</span><span class="n">NormalizeObservation</span><span class="p">)[</span><span class="mi">0</span><span class="p">]:</span>
            <span class="n">obs_normalized</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">locals</span><span class="p">[</span><span class="s1">&#39;new_obs&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">obs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_env</span><span class="o">.</span><span class="n">env_method</span><span class="p">(</span><span class="s1">&#39;get_unwrapped_obs&#39;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">variable</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">variables</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">record</span><span class="p">(</span>
                    <span class="s1">&#39;normalized_observation/&#39;</span> <span class="o">+</span> <span class="n">variable</span><span class="p">,</span> <span class="n">obs_normalized</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">record</span><span class="p">(</span>
                    <span class="s1">&#39;observation/&#39;</span> <span class="o">+</span> <span class="n">variable</span><span class="p">,</span> <span class="n">obs</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        <span class="c1"># Only original values</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">obs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">locals</span><span class="p">[</span><span class="s1">&#39;new_obs&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">variable</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">variables</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">record</span><span class="p">(</span>
                    <span class="s1">&#39;observation/&#39;</span> <span class="o">+</span> <span class="n">variable</span><span class="p">,</span> <span class="n">obs</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>

        <span class="c1"># ACTION</span>
        <span class="n">variables</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_env</span><span class="o">.</span><span class="n">get_attr</span><span class="p">(</span><span class="s1">&#39;variables&#39;</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;action&#39;</span><span class="p">]</span>
        <span class="n">action</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="c1"># sinergym action received inner its own setpoints range</span>
        <span class="n">action_</span> <span class="o">=</span> <span class="n">info</span><span class="p">[</span><span class="s1">&#39;action_&#39;</span><span class="p">]</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># network output clipped with gym action space</span>
            <span class="n">action</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">locals</span><span class="p">[</span><span class="s1">&#39;clipped_actions&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">action</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">locals</span><span class="p">[</span><span class="s1">&#39;action&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Algorithm action key in locals dict unknown&#39;</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_env</span><span class="o">.</span><span class="n">get_attr</span><span class="p">(</span><span class="s1">&#39;flag_discrete&#39;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]:</span>
            <span class="n">action</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_env</span><span class="o">.</span><span class="n">get_attr</span><span class="p">(</span><span class="s1">&#39;action_mapping&#39;</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="n">action</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">variable</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">variables</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">action</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">record</span><span class="p">(</span>
                    <span class="s1">&#39;action/&#39;</span> <span class="o">+</span> <span class="n">variable</span><span class="p">,</span> <span class="n">action</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">record</span><span class="p">(</span>
                <span class="s1">&#39;action_simulation/&#39;</span> <span class="o">+</span> <span class="n">variable</span><span class="p">,</span> <span class="n">action_</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>

        <span class="c1"># Store episode data</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ep_rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">locals</span><span class="p">[</span><span class="s1">&#39;rewards&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">ep_rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">locals</span><span class="p">[</span><span class="s1">&#39;reward&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
            <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Algorithm reward key in locals dict unknown&#39;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">ep_powers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">info</span><span class="p">[</span><span class="s1">&#39;total_power&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ep_term_comfort</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">info</span><span class="p">[</span><span class="s1">&#39;comfort_penalty&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ep_term_energy</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">info</span><span class="p">[</span><span class="s1">&#39;total_power_no_units&#39;</span><span class="p">])</span>
        <span class="k">if</span><span class="p">(</span><span class="n">info</span><span class="p">[</span><span class="s1">&#39;comfort_penalty&#39;</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_comfort_violation</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ep_timesteps</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="c1"># If episode ends, store summary of episode and reset</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">done</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">locals</span><span class="p">[</span><span class="s1">&#39;dones&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">done</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">locals</span><span class="p">[</span><span class="s1">&#39;done&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Algorithm done key in locals dict unknown&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
            <span class="c1"># store last episode metrics</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">episode_metrics</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">episode_metrics</span><span class="p">[</span><span class="s1">&#39;ep_length&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ep_timesteps</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">episode_metrics</span><span class="p">[</span><span class="s1">&#39;cumulative_reward&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">ep_rewards</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">episode_metrics</span><span class="p">[</span><span class="s1">&#39;mean_reward&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ep_rewards</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">episode_metrics</span><span class="p">[</span><span class="s1">&#39;mean_power&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ep_powers</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">episode_metrics</span><span class="p">[</span><span class="s1">&#39;cumulative_power&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ep_powers</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">episode_metrics</span><span class="p">[</span><span class="s1">&#39;mean_comfort_penalty&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">ep_term_comfort</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">episode_metrics</span><span class="p">[</span><span class="s1">&#39;cumulative_comfort_penalty&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">ep_term_comfort</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">episode_metrics</span><span class="p">[</span><span class="s1">&#39;mean_power_penalty&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">ep_term_energy</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">episode_metrics</span><span class="p">[</span><span class="s1">&#39;cumulative_power_penalty&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">ep_term_energy</span><span class="p">)</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">episode_metrics</span><span class="p">[</span><span class="s1">&#39;comfort_violation_time(%)&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_comfort_violation</span> <span class="o">/</span> \
                    <span class="bp">self</span><span class="o">.</span><span class="n">ep_timesteps</span> <span class="o">*</span> <span class="mi">100</span>
            <span class="k">except</span> <span class="ne">ZeroDivisionError</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">episode_metrics</span><span class="p">[</span><span class="s1">&#39;comfort_violation_time(%)&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span>

            <span class="c1"># reset episode info</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ep_rewards</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ep_powers</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ep_term_comfort</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ep_term_energy</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ep_timesteps</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_comfort_violation</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="c1"># During first episode, as it not finished, it shouldn&#39;t be recording</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;episode_metrics&#39;</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">metric</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">episode_metrics</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">record</span><span class="p">(</span>
                    <span class="s1">&#39;episode/&#39;</span> <span class="o">+</span> <span class="n">key</span><span class="p">,</span> <span class="n">metric</span><span class="p">)</span>

        <span class="k">return</span> <span class="kc">True</span>

    <span class="k">def</span> <span class="nf">on_training_end</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">is_wrapped</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">training_env</span><span class="p">,</span> <span class="n">LoggerWrapper</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">training_env</span><span class="o">.</span><span class="n">env_method</span><span class="p">(</span><span class="s1">&#39;activate_logger&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You can specify if you want Sinergym logger (see <a class="reference internal" href="output.html#logger"><span class="std std-ref">Logger</span></a>) to record simulation interactions during training at the same time using <code class="docutils literal notranslate"><span class="pre">sinergym_logger</span></code> attribute in constructor.</p>
</div>
<p>This callback derives <code class="docutils literal notranslate"><span class="pre">BaseCallback</span></code> from Stable Baselines 3 while <code class="docutils literal notranslate"><span class="pre">BaseCallBack</span></code> uses <a class="reference external" href="https://www.tensorflow.org/tensorboard?hl=es-419">Tensorboard</a> on the background at the same time.
With Tensorboard, it's possible to visualize all DRL training in real time and compare between different executions. This is an example:</p>
<a class="reference internal image-reference" href="../_images/tensorboard_example.png"><img alt="Tensorboard example" class="align-center" src="../_images/tensorboard_example.png" style="width: 800px;" /></a>
<p>There are tables which are in some algorithms and not in others and vice versa. It is important the difference between <code class="docutils literal notranslate"><span class="pre">OnPolicyAlgorithms</span></code> and <code class="docutils literal notranslate"><span class="pre">OffPolicyAlgorithms</span></code>:</p>
<ul class="simple">
<li><p><strong>OnPolicyAlgorithms</strong> can be recorded each timestep, we can set a <code class="docutils literal notranslate"><span class="pre">log_interval</span></code> in learn process in order to specify the step frequency log.</p></li>
<li><p><strong>OffPolicyAlgorithms</strong> can be recorded each episode. Consequently, <code class="docutils literal notranslate"><span class="pre">log_interval</span></code> in learn process is used to specify the episode frequency log and not step frequency.
Some features like actions and observations are set up in each timestep. Thus, Off Policy Algorithms record a mean value of whole episode values instead of values steps by steps (see <code class="docutils literal notranslate"><span class="pre">LoggerCallback</span></code> class implementation).</p></li>
</ul>
<section id="tensorboard-structure">
<h3>Tensorboard structure<a class="headerlink" href="#tensorboard-structure" title="Permalink to this headline"></a></h3>
<p>The main structure for Sinergym with Tensorboard is:</p>
<ul class="simple">
<li><p><strong>action</strong>: This section has action values during training. When algorithm is On Policy, it will appear <strong>action_simulation</strong> too. This is because algorithms
in continuous environments has their own output and clipped with gym action space. Then, this output is parse to simulation action space (See <a class="reference internal" href="environments.html#observation-action-spaces"><span class="std std-ref">Observation/action spaces</span></a>).</p></li>
<li><dl class="simple">
<dt><strong>episode</strong>: Here is stored all information about entire episodes. It is equivalent to progress.csv in Sinergym logger (see Sinergym <a class="reference internal" href="output.html#output-format"><span class="std std-ref">Output format</span></a>):</dt><dd><ul>
<li><p><em>comfort_violation_time(%)</em>: Percentage of time in episode simulation in which temperature has been out of bound comfort temperature ranges.</p></li>
<li><p><em>cumulative_comfort_penalty</em>: Sum of comfort penalties (reward component) during whole episode.</p></li>
<li><p><em>cumulative_power</em>: Sum of power consumption during whole episode.</p></li>
<li><p><em>cumulative_power_penalty</em>: Sum of power penalties (reward component) during whole episode.</p></li>
<li><p><em>cumulative_reward</em>: Sum of reward during whole episode.</p></li>
<li><p><em>ep_length</em>: Timesteps executed in each episode.</p></li>
<li><p><em>mean_comfort_penalty</em>: Mean comfort penalty per step in episode.</p></li>
<li><p><em>mean_power</em>: Mean power consumption per step in episode.</p></li>
<li><p><em>mean_power_penalty</em>: Mean power penalty per step in episode.</p></li>
<li><p><em>mean_reward</em>: Mean reward obtained per step in episode.</p></li>
</ul>
</dd>
</dl>
</li>
<li><p><strong>observation</strong>: Here is recorded all observation values during simulation. This values depends on the environment which is being simulated (see <a class="reference internal" href="output.html#output-format"><span class="std std-ref">Output format</span></a>).</p></li>
<li><p><strong>normalized_observation</strong> (optional): This section appear only when environment has been wrapped with normalization (see <a class="reference internal" href="wrappers.html#wrappers"><span class="std std-ref">Wrappers</span></a>). The model will train with this normalized values and they will be recorded both; original observation and normalized observation.</p></li>
<li><p><strong>rollout</strong>: Algorithm metrics in Stable Baselines by default. For example, DQN has <em>exploration_rate</em> and this value doesn't appear in other algorithms.</p></li>
<li><p><strong>time</strong>: Monitoring time of execution.</p></li>
<li><p><strong>train</strong>: Record specific neural network information for each algorithm, provided by Stable baselines as well as rollout.</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Evaluation of models can be recorded too, adding <code class="docutils literal notranslate"><span class="pre">EvalLoggerCallback</span></code> to model learn method.</p>
</div>
</section>
</section>
<section id="how-use">
<h2>How use<a class="headerlink" href="#how-use" title="Permalink to this headline"></a></h2>
<p>You can try your own experiments and benefit from this functionality. <a class="reference external" href="https://github.com/jajimer/sinergym/blob/main/examples/DRL_usage.py">sinergym/examples/DRL_usage.py</a>
is a example code to use it. You can use directly DRL_battery.py directly from your local computer specifying <code class="docutils literal notranslate"><span class="pre">--tensorboard</span></code> flag in execution.</p>
<p>The most important information you must keep in mind when you try your own experiments are:</p>
<ul class="simple">
<li><p>Model is constructed with a algorithm constructor. Each algorithm can use its particular parameters.</p></li>
<li><p>If you wrapper environment with normalization, models will train with those normalized values.</p></li>
<li><p>Callbacks can be concatenated in a <code class="docutils literal notranslate"><span class="pre">CallbackList</span></code> instance from Stable Baselines 3.</p></li>
<li><p>Neural network will not train until you execute <code class="docutils literal notranslate"><span class="pre">model.learn()</span></code> method. Here is where you
specify train <code class="docutils literal notranslate"><span class="pre">timesteps</span></code>, <code class="docutils literal notranslate"><span class="pre">callbacks</span></code> and <code class="docutils literal notranslate"><span class="pre">log_interval</span></code> as we commented in type algorithms (On and Off Policy).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">DRL_usage.py</span></code> or <code class="docutils literal notranslate"><span class="pre">DRL_battery.py</span></code> requires some extra arguments to being executed like <code class="docutils literal notranslate"><span class="pre">-env</span></code> and <code class="docutils literal notranslate"><span class="pre">-ep</span></code>.</p></li>
</ul>
<p>Code example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">gym</span>
<span class="kn">import</span> <span class="nn">argparse</span>
<span class="kn">import</span> <span class="nn">mlflow</span>

<span class="kn">from</span> <span class="nn">sinergym.utils.callbacks</span> <span class="kn">import</span> <span class="n">LoggerCallback</span><span class="p">,</span> <span class="n">LoggerEvalCallback</span>
<span class="kn">from</span> <span class="nn">sinergym.utils.wrappers</span> <span class="kn">import</span> <span class="n">NormalizeObservation</span>


<span class="kn">from</span> <span class="nn">stable_baselines3.common.noise</span> <span class="kn">import</span> <span class="n">NormalActionNoise</span><span class="p">,</span> <span class="n">OrnsteinUhlenbeckActionNoise</span>
<span class="kn">from</span> <span class="nn">stable_baselines3</span> <span class="kn">import</span> <span class="n">A2C</span><span class="p">,</span> <span class="n">DDPG</span><span class="p">,</span> <span class="n">DQN</span><span class="p">,</span> <span class="n">PPO</span><span class="p">,</span> <span class="n">SAC</span>
<span class="kn">from</span> <span class="nn">stable_baselines3.common.callbacks</span> <span class="kn">import</span> <span class="n">EvalCallback</span><span class="p">,</span> <span class="n">BaseCallback</span><span class="p">,</span> <span class="n">CallbackList</span>
<span class="kn">from</span> <span class="nn">stable_baselines3.common.vec_env</span> <span class="kn">import</span> <span class="n">DummyVecEnv</span>


<span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">()</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--environment&#39;</span><span class="p">,</span> <span class="s1">&#39;-env&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--episodes&#39;</span><span class="p">,</span> <span class="s1">&#39;-ep&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--learning_rate&#39;</span><span class="p">,</span> <span class="s1">&#39;-lr&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">.0007</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--n_steps&#39;</span><span class="p">,</span> <span class="s1">&#39;-n&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--gamma&#39;</span><span class="p">,</span> <span class="s1">&#39;-g&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">.99</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--gae_lambda&#39;</span><span class="p">,</span> <span class="s1">&#39;-gl&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--ent_coef&#39;</span><span class="p">,</span> <span class="s1">&#39;-ec&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--vf_coef&#39;</span><span class="p">,</span> <span class="s1">&#39;-v&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">.5</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--max_grad_norm&#39;</span><span class="p">,</span> <span class="s1">&#39;-m&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">.5</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--rms_prop_eps&#39;</span><span class="p">,</span> <span class="s1">&#39;-rms&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">)</span>
<span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>

<span class="c1"># experiment ID</span>
<span class="n">environment</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">environment</span>
<span class="n">n_episodes</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">episodes</span>
<span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;A2C-&#39;</span> <span class="o">+</span> <span class="n">environment</span> <span class="o">+</span> <span class="s1">&#39;-&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">n_episodes</span><span class="p">)</span> <span class="o">+</span> <span class="s1">&#39;-episodes&#39;</span>

<span class="k">with</span> <span class="n">mlflow</span><span class="o">.</span><span class="n">start_run</span><span class="p">(</span><span class="n">run_name</span><span class="o">=</span><span class="n">name</span><span class="p">):</span>

    <span class="n">mlflow</span><span class="o">.</span><span class="n">log_param</span><span class="p">(</span><span class="s1">&#39;env&#39;</span><span class="p">,</span> <span class="n">environment</span><span class="p">)</span>
    <span class="n">mlflow</span><span class="o">.</span><span class="n">log_param</span><span class="p">(</span><span class="s1">&#39;episodes&#39;</span><span class="p">,</span> <span class="n">n_episodes</span><span class="p">)</span>

    <span class="n">mlflow</span><span class="o">.</span><span class="n">log_param</span><span class="p">(</span><span class="s1">&#39;learning_rate&#39;</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">)</span>
    <span class="n">mlflow</span><span class="o">.</span><span class="n">log_param</span><span class="p">(</span><span class="s1">&#39;n_steps&#39;</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">n_steps</span><span class="p">)</span>
    <span class="n">mlflow</span><span class="o">.</span><span class="n">log_param</span><span class="p">(</span><span class="s1">&#39;gamma&#39;</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">gamma</span><span class="p">)</span>
    <span class="n">mlflow</span><span class="o">.</span><span class="n">log_param</span><span class="p">(</span><span class="s1">&#39;gae_lambda&#39;</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">gae_lambda</span><span class="p">)</span>
    <span class="n">mlflow</span><span class="o">.</span><span class="n">log_param</span><span class="p">(</span><span class="s1">&#39;ent_coef&#39;</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">ent_coef</span><span class="p">)</span>
    <span class="n">mlflow</span><span class="o">.</span><span class="n">log_param</span><span class="p">(</span><span class="s1">&#39;vf_coef&#39;</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">vf_coef</span><span class="p">)</span>
    <span class="n">mlflow</span><span class="o">.</span><span class="n">log_param</span><span class="p">(</span><span class="s1">&#39;max_grad_norm&#39;</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">max_grad_norm</span><span class="p">)</span>
    <span class="n">mlflow</span><span class="o">.</span><span class="n">log_param</span><span class="p">(</span><span class="s1">&#39;rms_prop_eps&#39;</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">rms_prop_eps</span><span class="p">)</span>

    <span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="n">environment</span><span class="p">)</span>
    <span class="n">env</span> <span class="o">=</span> <span class="n">NormalizeObservation</span><span class="p">(</span><span class="n">env</span><span class="p">)</span>

    <span class="c1">#### TRAINING ####</span>

    <span class="c1"># Build model</span>

    <span class="n">model</span> <span class="o">=</span> <span class="n">A2C</span><span class="p">(</span><span class="s1">&#39;MlpPolicy&#39;</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">learning_rate</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">,</span>
                 <span class="n">n_steps</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">n_steps</span><span class="p">,</span>
                 <span class="n">gamma</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">gamma</span><span class="p">,</span>
                 <span class="n">gae_lambda</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">gae_lambda</span><span class="p">,</span>
                 <span class="n">ent_coef</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">ent_coef</span><span class="p">,</span>
                 <span class="n">vf_coef</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">vf_coef</span><span class="p">,</span>
                 <span class="n">max_grad_norm</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">max_grad_norm</span><span class="p">,</span>
                 <span class="n">rms_prop_eps</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">rms_prop_eps</span><span class="p">,</span>
                 <span class="n">tensorboard_log</span><span class="o">=</span><span class="s1">&#39;./tensorboard_log/&#39;</span><span class="p">)</span>


    <span class="n">n_timesteps_episode</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">simulator</span><span class="o">.</span><span class="n">_eplus_one_epi_len</span> <span class="o">/</span> \
        <span class="n">env</span><span class="o">.</span><span class="n">simulator</span><span class="o">.</span><span class="n">_eplus_run_stepsize</span>
    <span class="n">timesteps</span> <span class="o">=</span> <span class="n">n_episodes</span> <span class="o">*</span> <span class="n">n_timesteps_episode</span>

    <span class="n">env</span> <span class="o">=</span> <span class="n">DummyVecEnv</span><span class="p">([</span><span class="k">lambda</span><span class="p">:</span> <span class="n">env</span><span class="p">])</span>

    <span class="c1"># Callbacks</span>
    <span class="n">freq</span> <span class="o">=</span> <span class="mi">2</span>  <span class="c1"># evaluate every N episodes</span>
    <span class="n">eval_callback</span> <span class="o">=</span> <span class="n">LoggerEvalCallback</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">best_model_save_path</span><span class="o">=</span><span class="s1">&#39;./best_models/&#39;</span> <span class="o">+</span> <span class="n">name</span> <span class="o">+</span> <span class="s1">&#39;/&#39;</span><span class="p">,</span>
                                    <span class="n">log_path</span><span class="o">=</span><span class="s1">&#39;./best_models/&#39;</span> <span class="o">+</span> <span class="n">name</span> <span class="o">+</span> <span class="s1">&#39;/&#39;</span><span class="p">,</span> <span class="n">eval_freq</span><span class="o">=</span><span class="n">n_timesteps_episode</span> <span class="o">*</span> <span class="n">freq</span><span class="p">,</span>
                                    <span class="n">deterministic</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">render</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">n_eval_episodes</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">log_callback</span> <span class="o">=</span> <span class="n">LoggerCallback</span><span class="p">(</span><span class="n">sinergym_logger</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">callback</span> <span class="o">=</span> <span class="n">CallbackList</span><span class="p">([</span><span class="n">log_callback</span><span class="p">,</span> <span class="n">eval_callback</span><span class="p">])</span>

    <span class="c1"># Training</span>
    <span class="n">model</span><span class="o">.</span><span class="n">learn</span><span class="p">(</span><span class="n">total_timesteps</span><span class="o">=</span><span class="n">timesteps</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="n">callback</span><span class="p">,</span> <span class="n">log_interval</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="mlflow">
<h2>Mlflow<a class="headerlink" href="#mlflow" title="Permalink to this headline"></a></h2>
<p>As you have been able to see in usage examples, it is using <a class="reference external" href="https://mlflow.org/">Mlflow</a> in order to tracking experiments and recorded them methodically. It is recommended to use it.
You can start a local server with information stored during the battery of experiments such as initial and ending date of execution, hyperparameters, duration, etc.
Here is an example:</p>
<a class="reference internal image-reference" href="../_images/mlflow_example.png"><img alt="Tensorboard example" class="align-center" src="../_images/mlflow_example.png" style="width: 800px;" /></a>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For information about how use <em>Tensorboard</em> and <em>Mlflow</em> with a Cloud Computing paradigm, see <a class="reference internal" href="gcloudAPI.html#remote-tensorboard-log"><span class="std std-ref">Remote Tensorboard log</span></a> and <a class="reference internal" href="gcloudAPI.html#mlflow-tracking-server-set-up"><span class="std std-ref">Mlflow tracking server set up</span></a></p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><em>This is a work in progress project. Compatibility with others algorithms is being planned for the future!</em></p>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="wrappers.html" class="btn btn-neutral float-left" title="Wrappers" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="gcloudAPI.html" class="btn btn-neutral float-right" title="Sinergym with Google Cloud" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, J. Jiménez, J. Gómez, M. Molina, A. Manjavacas, A. Campoy.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>