{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Wrappers example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In this notebook, we will explore *Sinergym*'s pre-defined wrappers and how to use them.\n",
    "\n",
    "You can also create your own wrappers by inheriting from *gym.Wrapper* or any of its variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "import sinergym\n",
    "from sinergym.utils.wrappers import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-objective wrapper"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[MO-Gymnasium](https://github.com/Farama-Foundation/MO-Gymnasium) is an open-source Python library for developing and comparing multi-objective reinforcement learning algorithms. \n",
    "\n",
    "Available MO-Gymnasium environments return a reward vector instead of a scalar value, one for each objective.\n",
    "\n",
    "This wrapper enables *Sinergym* to return a reward vector. This way, *Sinergym* is made compatible with both multi-objective algorithms and algorithms that work with a traditional reward value.\n",
    "\n",
    "We can transform the returned reward into a vector using as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Eplus-5zone-hot-discrete-v1')\n",
    "env = MultiObjectiveReward(env, reward_terms=['energy_term', 'comfort_term'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that `reward_terms` are available in the `info` dict returned by the environment's `step` method. Otherwise, an execution error will occur.\n",
    "\n",
    "By default, *Sinergym* environments return all reward terms of the reward class in the `info` dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "action = env.action_space.sample()\n",
    "obs, reward, terminated, truncated, info = env.step(action)\n",
    "env.close()\n",
    "\n",
    "print(reward)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Previous observation wrappers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This wrapper will add previous timestep observation values to the current environment observation. \n",
    "\n",
    "You can select the variables whose previous observed values should be tracked. The observation space will be updated with the corresponding new dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Eplus-5zone-hot-discrete-v1')\n",
    "env = PreviousObservationWrapper(env, previous_variables=[\n",
    "    'htg_setpoint',\n",
    "    'clg_setpoint',\n",
    "    'air_temperature'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see how the observation values have been updated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "obs, _, _, _, _ = env.step(env.action_space.sample())\n",
    "obs_dict = dict(zip(env.get_wrapper_attr('observation_variables'), obs))\n",
    "env.close()\n",
    "\n",
    "print('NEW OBSERVATION: ', obs_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datetime wrapper"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This wrapper will replace the `day` value with the `is_weekend` flag, and `hour` and `month` with codified *sin* and *cos* values.\n",
    "\n",
    "The observation space is also automatically updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Eplus-5zone-hot-discrete-v1')\n",
    "env = DatetimeWrapper(env)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This wrapper removes the observation variables `month`, `day`, and `hour`, and replace them by `month_sin`, `month_cos`, `is_weekend`, `hour_sin`, and `hour_cos`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "obs, _, _, _, _ = env.step(env.action_space.sample())\n",
    "obs_dict = dict(zip(env.get_wrapper_attr('observation_variables'), obs))\n",
    "env.close()\n",
    "print('NEW OBSERVATION: ', obs_dict)\n",
    "\n",
    "print('NEW OBSERVATION: ', obs_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action normalization wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of how to normalize a continuous action space using the `NormalizeAction` wrapper.\n",
    "\n",
    "If the normalization range is not defined, it will be `[-1,1]` by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a continuous environment\n",
    "env = gym.make('Eplus-5zone-hot-continuous-v1')\n",
    "print('ORIGINAL ACTION SPACE: ', env.get_wrapper_attr('action_space'))\n",
    "\n",
    "# Apply the normalization wrapper\n",
    "env = NormalizeAction(env, normalize_range=(-1.0, 1.0))\n",
    "print('WRAPPED ACTION SPACE: ', env.get_wrapper_attr('action_space'))\n",
    "\n",
    "env.reset()\n",
    "for i in range(5):\n",
    "    action = env.action_space.sample()\n",
    "    print('Normalized action: ', action)\n",
    "    _, _, _, _, info = env.step(action)\n",
    "    print('Action performed in the simulator: ', info['action'])\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action discretization wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how to discretize a continuous action space. We will need to specify the **new discrete action space** and an **action mapping function** whose output matches the original unwrapped action space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will create a continuous environment\n",
    "env = gym.make('Eplus-5zone-hot-continuous-v1')\n",
    "print('ORIGINAL ACTION SPACE: ', env.get_wrapper_attr('action_space'))\n",
    "print('IS DISCRETE?: ', env.get_wrapper_attr('is_discrete'))\n",
    "\n",
    "# Defining new discrete space and action mapping function\n",
    "new_discrete_space = gym.spaces.Discrete(10)  # Action values [0,9]\n",
    "\n",
    "\n",
    "def action_mapping_function(action):\n",
    "    mapping = {\n",
    "        0: [15, 30],  # These lists match with the original action space\n",
    "        1: [16, 29],\n",
    "        2: [17, 28],\n",
    "        3: [18, 27],\n",
    "        4: [19, 26],\n",
    "        5: [20, 25],\n",
    "        6: [21, 24],\n",
    "        7: [22, 23],\n",
    "        8: [22, 22.5],\n",
    "        9: [21, 22.5]\n",
    "    }\n",
    "\n",
    "    return mapping[action]\n",
    "\n",
    "\n",
    "# Apply the discretization wrapper\n",
    "env = DiscretizeEnv(env, discrete_space=new_discrete_space,\n",
    "                    action_mapping=action_mapping_function)\n",
    "print('WRAPPED ACTION SPACE: ', env.get_wrapper_attr('action_space'))\n",
    "print('IS DISCRETE?: ', env.get_wrapper_attr('is_discrete'))\n",
    "env.reset()\n",
    "for i in range(5):\n",
    "    action = env.action_space.sample()\n",
    "    print('ACTION DISCRETE: ', action)\n",
    "    _, _, _, _, info = env.step(action)\n",
    "    print('Action done in simulator: ', info['action'])\n",
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete incremental wrapper"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This wrapper updates an environment to utilize an incremental setpoint action space.It converts the environment into a *discrete* environment with an action mapping function and action space depending on the `step` and `delta` values specified. \n",
    "\n",
    "The action is added to the **current setpoint** value instead of overwriting the latest action. Thus, the action is the current setpoint values with the applied increment/decrement, rather than the discrete value action that defines the increment/decrement itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Eplus-5zone-hot-continuous-v1')\n",
    "print('ORIGINAL ACTION SPACE: ', env.get_wrapper_attr('action_space'))\n",
    "\n",
    "env = DiscreteIncrementalWrapper(\n",
    "    env, initial_values=[21.0, 25.0], delta_temp=2, step_temp=0.5)\n",
    "\n",
    "print('WRAPPED ACTION SPACE: ', env.get_wrapper_attr('action_space'))\n",
    "print('WRAPPED ACTION MAPPING: ', env.get_wrapper_attr('action_mapping'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum and minimum values defined when creating the action mapping are read from the environment action space, ensuring that the setpoint increments and decrements do not exceed the corresponding limits.\n",
    "\n",
    "The `delta` and `step` values are used to determine how the discrete space of these increments and decrements will be constructed.\n",
    "\n",
    "Here's an example of how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "print('CURRENT SETPOINTS VALUES: ', env.get_wrapper_attr('current_setpoints'))\n",
    "\n",
    "for i in range(5):\n",
    "    action = env.action_space.sample()\n",
    "    _, _, _, _, info = env.step(action)\n",
    "    print('Action number ', i, ': ',\n",
    "          env.get_wrapper_attr('action_mapping')(action))\n",
    "    print('Setpoints update: ', info['action'])\n",
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization wrapper"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This wrapper is used to transform observations received from the simulator to values between in range `[-1, 1]`.\n",
    "\n",
    "It is based on [Gymnasium's dynamic normalization wrapper](https://gymnasium.farama.org/_modules/gymnasium/wrappers/normalize/#NormalizeObservation). \n",
    "\n",
    "Until properly calibrated, it may not be precise, and the values may often be out of range, so use this wrapper with caution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Original env\n",
    "env = gym.make('Eplus-5zone-hot-discrete-v1')\n",
    "\n",
    "# Normalized env\n",
    "env = NormalizeObservation(\n",
    "    env=env)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check how the specified variables have been correctly normalized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "obs, _, _, _, _ = env.step(env.action_space.sample())\n",
    "obs_dict = dict(zip(env.get_wrapper_attr('observation_variables'), obs))\n",
    "env.close()\n",
    "\n",
    "print('OBSERVATION WITH NORMALIZATION: ', obs_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging and storing data with logger wrappers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoggerWrapper layer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This wrapper uses *Sinergym*'s `LoggerStorage` class to properly capture the interaction flow with the environment.\n",
    "\n",
    "The class used by the wrapper can be replaced with a different back-end. It can then be combined with different wrappers to save the stored data, such as `CSVLogger` or `WandBLogger`. For more information about *Sinergym*'s logger, visit [Logging System Overview](https://ugr-sail.github.io/sinergym/compilation/main/pages/logging.html#logging-system-overview), [Logger Wrappers](https://ugr-sail.github.io/sinergym/compilation/main/pages/wrappers.html#logger-wrappers) and [an example about custom loggers](https://ugr-sail.github.io/sinergym/compilation/main/pages/notebooks/personalize_loggerwrapper.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Eplus-5zone-hot-discrete-v1')\n",
    "env = LoggerWrapper(env, storage_class=LoggerStorage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This wrapper enables the use of a `LoggerStorage` instance within the environment class and automatically captures interaction data while actions are sent by an agent. At each reset, the data from this class is cleared to start the next episode. The idea is to combine it with other output loggers like those listed below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoggerCSV layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CSVLogger(env)\n",
    "\n",
    "env.reset()\n",
    "truncated = terminated = False\n",
    "current_month = 0\n",
    "\n",
    "while not (terminated or truncated):\n",
    "    a = env.action_space.sample()\n",
    "    _, _, terminated, truncated, _ = env.step(a)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the `LoggerWrapper` is applied, this wrapper can be used to output episode data through *Sinergym*â€™s output, along with summary metrics added to CSV files. More details on this structure can be found in [OutputFormat](https://ugr-sail.github.io/sinergym/compilation/main/pages/output.html). \n",
    "\n",
    "*Sinergym* will raise an error if this wrapper is used without first enabling `LoggerWrapper` or a similar custom logger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WandBLogger layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = WandBLogger(env = Env,\n",
    "#                 entity = <wandb_account_entity>,\n",
    "#                 project_name = <wandb_project_name>,\n",
    "#                 run_name = <run_name>\n",
    "#                 group = 'Notebook_example',\n",
    "#                 tags: ['tag1','tag2'],\n",
    "#                 save_code = False,\n",
    "#                 dump_frequency = 1000,\n",
    "#                 artifact_save = True,\n",
    "#                 artifact_type = 'output',\n",
    "#                 excluded_info_keys = ['reward',\n",
    "#                                   'action',\n",
    "#                                   'timestep',\n",
    "#                                   'month',\n",
    "#                                   'day',\n",
    "#                                   'hour',\n",
    "#                                   'time_elapsed(hours)',\n",
    "#                                   'reward_weight',\n",
    "#                                   'is_raining'],\n",
    "#                 excluded_episode_summary_keys = ['terminated',\n",
    "#                                              'truncated']):\n",
    "\n",
    "# env.reset()\n",
    "# truncated = terminated = False\n",
    "# current_month = 0\n",
    "# while not (terminated or truncated):\n",
    "#     a = env.action_space.sample()\n",
    "#     _,_,terminated,truncated,_=env.step(a)\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to `CSVLogger`, this wrapper requires the environment to have been previously encapsulated by a `LoggerWrapper` or any custom logger.\n",
    "\n",
    "The user must have a pre-existing **Weights and Biases** account and correctly configured it. \n",
    "\n",
    "This wrapper does not override `CSVLogger`, so both can be applied simultaneously."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-observation wrapper"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This wrapper stacks observations in a history queue, whose size can be customized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Original environment\n",
    "env = gym.make('Eplus-5zone-hot-discrete-v1')\n",
    "obs, info = env.reset()\n",
    "print('BEFORE MULTI OBSERVATION: ', obs)\n",
    "\n",
    "# Multi-observation environment with a queue of size 5\n",
    "env = MultiObsWrapper(env, n=5, flatten=True)\n",
    "obs, info = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MULTI OBSERVATION: \\n', obs)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather forecasting wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This wrapper adds weather forecast information to the current observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original environment\n",
    "env = gym.make('Eplus-5zone-hot-discrete-v1')\n",
    "obs, info = env.reset()\n",
    "print('OBSERVATION VARIABLES BEFORE WEATHER FORECASTING: ',\n",
    "      env.get_wrapper_attr('observation_variables'))\n",
    "print('OBSERVATION BEFORE WEATHER FORECASTING: ', obs)\n",
    "\n",
    "# Weather forecasting environment\n",
    "env = WeatherForecastingWrapper(env, n=5, delta=1)\n",
    "obs, info = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('OBSERVATION VARIABLES AFTER WEATHER FORECASTING: ',\n",
    "      env.get_wrapper_attr('observation_variables'))\n",
    "\n",
    "print('OBSERVATION AFTER WEATHER FORECASTING: ', obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Energy cost wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This wrapper adds energy cost information to the current observation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original environment\n",
    "env = gym.make('Eplus-5zone-hot-discrete-v1')\n",
    "obs, info = env.reset()\n",
    "print('OBSERVATION VARIABLES BEFORE ADDING ENERGY COST: \\n',\n",
    "      env.get_wrapper_attr('observation_variables'))\n",
    "print('OBSERVATION VALUES BEFORE ADDING ENERGY COST: \\n', obs)\n",
    "\n",
    "# Energy Cost environment\n",
    "env = EnergyCostWrapper(\n",
    "    env, energy_cost_data_path='/workspaces/sinergym/sinergym/data/energy_cost/PVPC_active_energy_billing_Iberian_Peninsula_2023.csv')\n",
    "obs, info = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('OBSERVATION VARIABLES AFTER ADDING ENERGY COST: \\n', env.get_wrapper_attr('observation_variables'))\n",
    "print('OBSERVATION VALUES AFTER ADDING ENERGY COST: \\n',obs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nesting wrappers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All wrappers included in *Sinergym* are stackable and organized in layers. However, the order in which these layers are applied can affect the final result, depending on the wrappers being used.\n",
    "\n",
    "For instance, applying the logger before normalizing differs from doing it in the reverse order. In the first case, the data will be logged without normalization, even though the agent will operate in a normalized environment. In the second case, the logger will capture the normalized values since it encapsulates the normalization applied by the previous layer.\n",
    "\n",
    "An example of how to nest wrappers is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Eplus-5zone-hot-continuous-v1')\n",
    "env = MultiObjectiveReward(\n",
    "    env=env,\n",
    "    reward_terms=[\n",
    "        'energy_term',\n",
    "        'comfort_term'])\n",
    "env = PreviousObservationWrapper(env, previous_variables=[\n",
    "    'htg_setpoint',\n",
    "    'clg_setpoint',\n",
    "    'air_temperature'])\n",
    "env = DatetimeWrapper(env)\n",
    "env = DiscreteIncrementalWrapper(\n",
    "    env, initial_values=[21.0, 25.0], delta_temp=2, step_temp=0.5)\n",
    "env = NormalizeObservation(\n",
    "    env=env)\n",
    "env = LoggerWrapper(env=env)\n",
    "env = MultiObsWrapper(env=env, n=5, flatten=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now we can simply use the wrapped environment as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    obs, info = env.reset()\n",
    "    truncated = terminated = False\n",
    "    current_month = 0\n",
    "    while not (terminated or truncated):\n",
    "        a = env.action_space.sample()\n",
    "        obs, reward, terminated, truncated, info = env.step(a)\n",
    "        if info['month'] != current_month:  # display results every month\n",
    "            current_month = info['month']\n",
    "            print('Reward: ', reward, info)\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
