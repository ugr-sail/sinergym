{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Default building control using an empty action space"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "It is possible to run a simulation using the default building control performed by EnergyPlus (as specified in the building model file).\n",
    "\n",
    "For instance, from the container's workspace, run the following command:\n",
    "\n",
    "\n",
    "```bash\n",
    "$ energyplus -w sinergym/data/weather/USA_PA_Pittsburgh-Allegheny.County.AP.725205_TMY3.epw sinergym/data/buildings/5ZoneAutoDXVAV.epJSON\n",
    "```\n",
    "\n",
    "However, doing this without our framework has some **drawbacks**:\n",
    "\n",
    "- You will only have the default EnergyPlus output and will not have access to **additional output information**, such as data provided by the logger wrapper, which tracks all the environment interactions. \n",
    "\n",
    "- Moreover, building models have a default ``Site:Location``and ``SizingPeriod:DesignDay``, which *Sinergym* automatically adjusts based on the specified weather, so you would need to manually modify these settings before launching the simulation.\n",
    "\n",
    "- Lastly, you would also need to manually adjust the *RunPeriod* in the building file before starting the simulation.\n",
    "\n",
    "Hence, to avoid manual configurations, we recommend setting up an **empty action interface** in a *Sinergym* environment. For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#==============================================================================================#\n",
      "\u001b[38;20m[ENVIRONMENT] (INFO) : Creating Gymnasium environment.\u001b[0m\n",
      "\u001b[38;20m[ENVIRONMENT] (INFO) : Name: office-hot-continuous-v1\u001b[0m\n",
      "#==============================================================================================#\n",
      "\u001b[38;20m[MODELING] (INFO) : Experiment working directory created.\u001b[0m\n",
      "\u001b[38;20m[MODELING] (INFO) : Working directory: /workspaces/sinergym/examples/Eplus-env-office-hot-continuous-v1-res1\u001b[0m\n",
      "\u001b[38;20m[MODELING] (INFO) : Model Config is correct.\u001b[0m\n",
      "\u001b[38;20m[MODELING] (INFO) : Update building model Output:Variable with variable names.\u001b[0m\n",
      "\u001b[38;20m[MODELING] (INFO) : Update building model Output:Meter with meter names.\u001b[0m\n",
      "\u001b[38;20m[MODELING] (INFO) : Runperiod established.\u001b[0m\n",
      "\u001b[38;20m[MODELING] (INFO) : Episode length (seconds): 31536000.0\u001b[0m\n",
      "\u001b[38;20m[MODELING] (INFO) : timestep size (seconds): 900.0\u001b[0m\n",
      "\u001b[38;20m[MODELING] (INFO) : timesteps per episode: 35040\u001b[0m\n",
      "\u001b[38;20m[REWARD] (INFO) : Reward function initialized.\u001b[0m\n",
      "\u001b[38;20m[ENVIRONMENT] (INFO) : Environment created successfully.\u001b[0m\n",
      "\u001b[38;20m[WRAPPER LoggerWrapper] (INFO) : Wrapper initialized.\u001b[0m\n",
      "#----------------------------------------------------------------------------------------------#\n",
      "\u001b[38;20m[ENVIRONMENT] (INFO) : Starting a new episode.\u001b[0m\n",
      "\u001b[38;20m[ENVIRONMENT] (INFO) : Episode 1: office-hot-continuous-v1\u001b[0m\n",
      "#----------------------------------------------------------------------------------------------#\n",
      "\u001b[38;20m[MODELING] (INFO) : Episode directory created.\u001b[0m\n",
      "\u001b[38;20m[MODELING] (INFO) : Weather file USA_AZ_Davis-Monthan.AFB.722745_TMY3.epw used.\u001b[0m\n",
      "\u001b[38;20m[MODELING] (INFO) : Adapting weather to building model.\u001b[0m\n",
      "\u001b[38;20m[ENVIRONMENT] (INFO) : Saving episode output path.\u001b[0m\n",
      "\u001b[38;20m[ENVIRONMENT] (INFO) : Episode 1 started.\u001b[0m\n",
      "\u001b[38;20m[SIMULATOR] (INFO) : handlers initialized.\u001b[0m\n",
      "\u001b[38;20m[SIMULATOR] (INFO) : handlers are ready.\u001b[0m\n",
      "\u001b[38;20m[SIMULATOR] (INFO) : System is ready.\u001b[0m\n",
      "Reward:  -3.400918408046664 {'time_elapsed(hours)': 0.5, 'month': 1, 'day': 1, 'hour': 0, 'is_raining': False, 'action': array([], dtype=float32), 'timestep': 1, 'reward': -3.400918408046664, 'energy_term': -0.008430450234156376, 'comfort_term': -3.3924879578125076, 'reward_weight': 0.5, 'abs_energy_penalty': -168.6090046831275, 'abs_comfort_penalty': -6.784975915625015, 'total_power_demand': 168.6090046831275, 'total_temperature_violation': 6.784975915625015}\n",
      "Simulation Progress [Episode 1]:  10%|█         | 10/100 [00:02<00:22,  3.94%/s, 10% completed] Reward:  -28187.63927138061 {'time_elapsed(hours)': 744.25, 'month': 2, 'day': 1, 'hour': 0, 'is_raining': False, 'action': array([], dtype=float32), 'timestep': 2976, 'reward': -11.285602485359158, 'energy_term': -0.010066938284793056, 'comfort_term': -11.275535547074366, 'reward_weight': 0.5, 'abs_energy_penalty': -201.33876569586113, 'abs_comfort_penalty': -22.551071094148732, 'total_power_demand': 201.33876569586113, 'total_temperature_violation': 22.551071094148732}\n",
      "Simulation Progress [Episode 1]:  17%|█▋        | 17/100 [00:03<00:14,  5.57%/s, 17% completed]Reward:  -45940.77927743045 {'time_elapsed(hours)': 1416.25, 'month': 3, 'day': 1, 'hour': 0, 'is_raining': False, 'action': array([], dtype=float32), 'timestep': 5664, 'reward': -10.268961643225715, 'energy_term': -0.010066938284793056, 'comfort_term': -10.258894704940923, 'reward_weight': 0.5, 'abs_energy_penalty': -201.33876569586113, 'abs_comfort_penalty': -20.517789409881846, 'total_power_demand': 201.33876569586113, 'total_temperature_violation': 20.517789409881846}\n",
      "Simulation Progress [Episode 1]:  26%|██▌       | 26/100 [00:05<00:14,  5.20%/s, 26% completed]Reward:  -70603.72337365196 {'time_elapsed(hours)': 2160.25, 'month': 4, 'day': 1, 'hour': 0, 'is_raining': False, 'action': array([], dtype=float32), 'timestep': 8640, 'reward': -12.211713512655699, 'energy_term': -0.010059948897039251, 'comfort_term': -12.20165356375866, 'reward_weight': 0.5, 'abs_energy_penalty': -201.198977940785, 'abs_comfort_penalty': -24.40330712751732, 'total_power_demand': 201.198977940785, 'total_temperature_violation': 24.40330712751732}\n",
      "Simulation Progress [Episode 1]:  34%|███▍      | 34/100 [00:07<00:15,  4.25%/s, 34% completed]Reward:  -114977.5022623719 {'time_elapsed(hours)': 2880.25, 'month': 5, 'day': 1, 'hour': 0, 'is_raining': False, 'action': array([], dtype=float32), 'timestep': 11520, 'reward': -19.68289174648665, 'energy_term': -0.010059948897039251, 'comfort_term': -19.67283179758961, 'reward_weight': 0.5, 'abs_energy_penalty': -201.198977940785, 'abs_comfort_penalty': -39.34566359517922, 'total_power_demand': 201.198977940785, 'total_temperature_violation': 39.34566359517922}\n",
      "Simulation Progress [Episode 1]:  42%|████▏     | 42/100 [00:09<00:14,  4.11%/s, 42% completed]Reward:  -167310.85629364094 {'time_elapsed(hours)': 3624.25, 'month': 6, 'day': 1, 'hour': 0, 'is_raining': False, 'action': array([], dtype=float32), 'timestep': 14496, 'reward': -2.21823982552994, 'energy_term': -0.008430450234156376, 'comfort_term': -2.2098093752957837, 'reward_weight': 0.5, 'abs_energy_penalty': -168.6090046831275, 'abs_comfort_penalty': -4.419618750591567, 'total_power_demand': 168.6090046831275, 'total_temperature_violation': 4.419618750591567}\n",
      "Simulation Progress [Episode 1]:  51%|█████     | 51/100 [00:11<00:13,  3.62%/s, 51% completed]Reward:  -181130.2400803524 {'time_elapsed(hours)': 4344.25, 'month': 7, 'day': 1, 'hour': 0, 'is_raining': False, 'action': array([], dtype=float32), 'timestep': 17376, 'reward': -8.064856313778423, 'energy_term': -0.26742679043554096, 'comfort_term': -7.797429523342881, 'reward_weight': 0.5, 'abs_energy_penalty': -5348.535808710819, 'abs_comfort_penalty': -15.594859046685762, 'total_power_demand': 5348.535808710819, 'total_temperature_violation': 15.594859046685762}\n",
      "Simulation Progress [Episode 1]:  59%|█████▉    | 59/100 [00:13<00:10,  3.93%/s, 59% completed]Reward:  -193401.17140870017 {'time_elapsed(hours)': 5088.25, 'month': 8, 'day': 1, 'hour': 0, 'is_raining': False, 'action': array([], dtype=float32), 'timestep': 20352, 'reward': -5.7091399347815965, 'energy_term': -0.010059948897039251, 'comfort_term': -5.699079985884557, 'reward_weight': 0.5, 'abs_energy_penalty': -201.198977940785, 'abs_comfort_penalty': -11.398159971769115, 'total_power_demand': 201.198977940785, 'total_temperature_violation': 11.398159971769115}\n",
      "Simulation Progress [Episode 1]:  67%|██████▋   | 67/100 [00:15<00:08,  3.93%/s, 67% completed]Reward:  -204732.49252537402 {'time_elapsed(hours)': 5832.25, 'month': 9, 'day': 1, 'hour': 0, 'is_raining': False, 'action': array([], dtype=float32), 'timestep': 23328, 'reward': -3.009932245152207, 'energy_term': -0.008430450234156376, 'comfort_term': -3.0015017949180507, 'reward_weight': 0.5, 'abs_energy_penalty': -168.6090046831275, 'abs_comfort_penalty': -6.003003589836101, 'total_power_demand': 168.6090046831275, 'total_temperature_violation': 6.003003589836101}\n",
      "Simulation Progress [Episode 1]:  76%|███████▌  | 76/100 [00:17<00:05,  4.19%/s, 76% completed]Reward:  -215571.57315607253 {'time_elapsed(hours)': 6552.25, 'month': 10, 'day': 1, 'hour': 0, 'is_raining': False, 'action': array([], dtype=float32), 'timestep': 26208, 'reward': -25.841002498884077, 'energy_term': -0.010059948897039251, 'comfort_term': -25.830942549987036, 'reward_weight': 0.5, 'abs_energy_penalty': -201.198977940785, 'abs_comfort_penalty': -51.66188509997407, 'total_power_demand': 201.198977940785, 'total_temperature_violation': 51.661885099974064}\n",
      "Simulation Progress [Episode 1]:  84%|████████▍ | 84/100 [00:19<00:03,  4.71%/s, 84% completed]Reward:  -258417.85317828832 {'time_elapsed(hours)': 7296.25, 'month': 11, 'day': 1, 'hour': 0, 'is_raining': False, 'action': array([], dtype=float32), 'timestep': 29184, 'reward': -14.908274454668001, 'energy_term': -0.11419542960070476, 'comfort_term': -14.794079025067298, 'reward_weight': 0.5, 'abs_energy_penalty': -2283.908592014095, 'abs_comfort_penalty': -29.588158050134595, 'total_power_demand': 2283.908592014095, 'total_temperature_violation': 29.588158050134595}\n",
      "Simulation Progress [Episode 1]:  92%|█████████▏| 92/100 [00:21<00:01,  4.88%/s, 92% completed]Reward:  -289838.39189161843 {'time_elapsed(hours)': 8016.25, 'month': 12, 'day': 1, 'hour': 0, 'is_raining': False, 'action': array([], dtype=float32), 'timestep': 32064, 'reward': -7.385952598375827, 'energy_term': -0.008430450234156376, 'comfort_term': -7.37752214814167, 'reward_weight': 0.5, 'abs_energy_penalty': -168.6090046831275, 'abs_comfort_penalty': -14.75504429628334, 'total_power_demand': 168.6090046831275, 'total_temperature_violation': 14.75504429628334}\n",
      "Simulation Progress [Episode 1]: 100%|██████████| 100/100 [00:22<00:00,  5.03%/s, 100% completed]Episode  0 Mean reward:  -8.928154911945706 Cumulative reward:  -312842.54811457754\n",
      "Simulation Progress [Episode 1]: 100%|██████████| 100/100 [00:25<00:00,  4.00%/s, 100% completed]\n",
      "\u001b[38;20m[ENVIRONMENT] (INFO) : Environment closed. [office-hot-continuous-v1]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "import sinergym\n",
    "from sinergym.utils.wrappers import LoggerWrapper\n",
    "\n",
    "env = gym.make(\n",
    "    'Eplus-office-hot-continuous-v1',\n",
    "    actuators={},\n",
    "    action_space=gym.spaces.Box(\n",
    "        low=0,\n",
    "        high=0,\n",
    "        shape=(0,)))\n",
    "env = LoggerWrapper(env)\n",
    "\n",
    "for i in range(1):\n",
    "    obs, info = env.reset()\n",
    "    rewards = []\n",
    "    truncated = terminated = False\n",
    "    current_month = 0\n",
    "    while not (terminated or truncated):\n",
    "        a = env.action_space.sample()\n",
    "        obs, reward, terminated, truncated, info = env.step(a)\n",
    "        rewards.append(reward)\n",
    "        if info['month'] != current_month:  # display results every month\n",
    "            current_month = info['month']\n",
    "            print('Reward: ', sum(rewards), info)\n",
    "    print(\n",
    "        'Episode ',\n",
    "        i,\n",
    "        'Mean reward: ',\n",
    "        np.mean(rewards),\n",
    "        'Cumulative reward: ',\n",
    "        sum(rewards))\n",
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, a default environment is created, but the space and definition of the default action **are replaced with an empty one**. *Sinergym* handles the necessary background changes. Then, the implemented random agent sends **empty actions (`[]`)** to the environment.\n",
    "\n",
    "When setting an empty action space, *Sinergym* retains the default actuators that are defined in the building model. Their complexity and implementation will depend on the building definition in the *epJSON* file.\n",
    "\n",
    "The benefits of this approach include the ability to **mix and match weathers and buildings** as desired, with *Sinergym* automatically making all the necessary configuration.\n",
    "\n",
    "You can simulate as many years as you want in a single experiment, using the pre-defined loggers.\n",
    "\n",
    "This method also provides more flexibility when choosing which observation variables are going to be used. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
